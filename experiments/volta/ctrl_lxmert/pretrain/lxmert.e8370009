WARNING:tensorflow:From /gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/tensorpack/callbacks/hooks.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

WARNING:tensorflow:From /gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/tensorpack/tfutils/optimizer.py:18: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/tensorpack/tfutils/sesscreate.py:20: The name tf.train.SessionCreator is deprecated. Please use tf.compat.v1.train.SessionCreator instead.

12/07/2020 09:07:45 - INFO - __main__ -   device: cuda n_gpu: 4, distributed training: False
12/07/2020 09:07:46 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/4/19ITA380/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
12/07/2020 09:07:49 - INFO - volta.train_utils -   logging file at: ../../logs/volta/conceptual_captions/ctrl_lxmert
12/07/2020 09:07:49 - INFO - volta.utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/4/19ITA380/.pytorch_pretrained_bert/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
12/07/2020 09:07:54 - INFO - volta.utils -   
12/07/2020 09:07:54 - INFO - volta.utils -   Weights of BertForVLPreTraining not initialized from pretrained model: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.ImgLayerNorm.weight', 'bert.v_embeddings.ImgLayerNorm.bias', 'bert.v_embeddings.LocLayerNorm.weight', 'bert.v_embeddings.LocLayerNorm.bias', 'bert.encoder.layer.0.attention_self.v_query.weight', 'bert.encoder.layer.0.attention_self.v_query.bias', 'bert.encoder.layer.0.attention_self.v_key.weight', 'bert.encoder.layer.0.attention_self.v_key.bias', 'bert.encoder.layer.0.attention_self.v_value.weight', 'bert.encoder.layer.0.attention_self.v_value.bias', 'bert.encoder.layer.0.attention_output.v_dense.weight', 'bert.encoder.layer.0.attention_output.v_dense.bias', 'bert.encoder.layer.0.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.0.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.1.intermediate.v_dense.weight', 'bert.encoder.layer.1.intermediate.v_dense.bias', 'bert.encoder.layer.1.output.v_dense.weight', 'bert.encoder.layer.1.output.v_dense.bias', 'bert.encoder.layer.1.output.v_LayerNorm.weight', 'bert.encoder.layer.1.output.v_LayerNorm.bias', 'bert.encoder.layer.2.attention_self.v_query.weight', 'bert.encoder.layer.2.attention_self.v_query.bias', 'bert.encoder.layer.2.attention_self.v_key.weight', 'bert.encoder.layer.2.attention_self.v_key.bias', 'bert.encoder.layer.2.attention_self.v_value.weight', 'bert.encoder.layer.2.attention_self.v_value.bias', 'bert.encoder.layer.2.attention_output.v_dense.weight', 'bert.encoder.layer.2.attention_output.v_dense.bias', 'bert.encoder.layer.2.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.2.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.3.intermediate.v_dense.weight', 'bert.encoder.layer.3.intermediate.v_dense.bias', 'bert.encoder.layer.3.output.v_dense.weight', 'bert.encoder.layer.3.output.v_dense.bias', 'bert.encoder.layer.3.output.v_LayerNorm.weight', 'bert.encoder.layer.3.output.v_LayerNorm.bias', 'bert.encoder.layer.4.attention_self.v_query.weight', 'bert.encoder.layer.4.attention_self.v_query.bias', 'bert.encoder.layer.4.attention_self.v_key.weight', 'bert.encoder.layer.4.attention_self.v_key.bias', 'bert.encoder.layer.4.attention_self.v_value.weight', 'bert.encoder.layer.4.attention_self.v_value.bias', 'bert.encoder.layer.4.attention_output.v_dense.weight', 'bert.encoder.layer.4.attention_output.v_dense.bias', 'bert.encoder.layer.4.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.4.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.5.intermediate.v_dense.weight', 'bert.encoder.layer.5.intermediate.v_dense.bias', 'bert.encoder.layer.5.output.v_dense.weight', 'bert.encoder.layer.5.output.v_dense.bias', 'bert.encoder.layer.5.output.v_LayerNorm.weight', 'bert.encoder.layer.5.output.v_LayerNorm.bias', 'bert.encoder.layer.6.attention_self.v_query.weight', 'bert.encoder.layer.6.attention_self.v_query.bias', 'bert.encoder.layer.6.attention_self.v_key.weight', 'bert.encoder.layer.6.attention_self.v_key.bias', 'bert.encoder.layer.6.attention_self.v_value.weight', 'bert.encoder.layer.6.attention_self.v_value.bias', 'bert.encoder.layer.6.attention_output.v_dense.weight', 'bert.encoder.layer.6.attention_output.v_dense.bias', 'bert.encoder.layer.6.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.6.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.7.intermediate.v_dense.weight', 'bert.encoder.layer.7.intermediate.v_dense.bias', 'bert.encoder.layer.7.output.v_dense.weight', 'bert.encoder.layer.7.output.v_dense.bias', 'bert.encoder.layer.7.output.v_LayerNorm.weight', 'bert.encoder.layer.7.output.v_LayerNorm.bias', 'bert.encoder.layer.8.attention_self.v_query.weight', 'bert.encoder.layer.8.attention_self.v_query.bias', 'bert.encoder.layer.8.attention_self.v_key.weight', 'bert.encoder.layer.8.attention_self.v_key.bias', 'bert.encoder.layer.8.attention_self.v_value.weight', 'bert.encoder.layer.8.attention_self.v_value.bias', 'bert.encoder.layer.8.attention_output.v_dense.weight', 'bert.encoder.layer.8.attention_output.v_dense.bias', 'bert.encoder.layer.8.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.8.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.9.intermediate.v_dense.weight', 'bert.encoder.layer.9.intermediate.v_dense.bias', 'bert.encoder.layer.9.output.v_dense.weight', 'bert.encoder.layer.9.output.v_dense.bias', 'bert.encoder.layer.9.output.v_LayerNorm.weight', 'bert.encoder.layer.9.output.v_LayerNorm.bias', 'bert.encoder.layer.18.attention_self.query.weight', 'bert.encoder.layer.18.attention_self.query.bias', 'bert.encoder.layer.18.attention_self.key.weight', 'bert.encoder.layer.18.attention_self.key.bias', 'bert.encoder.layer.18.attention_self.value.weight', 'bert.encoder.layer.18.attention_self.value.bias', 'bert.encoder.layer.18.attention_self.v_query.weight', 'bert.encoder.layer.18.attention_self.v_query.bias', 'bert.encoder.layer.18.attention_self.v_key.weight', 'bert.encoder.layer.18.attention_self.v_key.bias', 'bert.encoder.layer.18.attention_self.v_value.weight', 'bert.encoder.layer.18.attention_self.v_value.bias', 'bert.encoder.layer.18.attention_output.dense.weight', 'bert.encoder.layer.18.attention_output.dense.bias', 'bert.encoder.layer.18.attention_output.LayerNorm.weight', 'bert.encoder.layer.18.attention_output.LayerNorm.bias', 'bert.encoder.layer.18.attention_output.v_dense.weight', 'bert.encoder.layer.18.attention_output.v_dense.bias', 'bert.encoder.layer.18.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.18.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.19.attention_self.v_query.weight', 'bert.encoder.layer.19.attention_self.v_query.bias', 'bert.encoder.layer.19.attention_self.v_key.weight', 'bert.encoder.layer.19.attention_self.v_key.bias', 'bert.encoder.layer.19.attention_self.v_value.weight', 'bert.encoder.layer.19.attention_self.v_value.bias', 'bert.encoder.layer.19.attention_output.v_dense.weight', 'bert.encoder.layer.19.attention_output.v_dense.bias', 'bert.encoder.layer.19.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.19.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.20.intermediate.v_dense.weight', 'bert.encoder.layer.20.intermediate.v_dense.bias', 'bert.encoder.layer.20.output.v_dense.weight', 'bert.encoder.layer.20.output.v_dense.bias', 'bert.encoder.layer.20.output.v_LayerNorm.weight', 'bert.encoder.layer.20.output.v_LayerNorm.bias', 'bert.encoder.layer.21.attention_self.query.weight', 'bert.encoder.layer.21.attention_self.query.bias', 'bert.encoder.layer.21.attention_self.key.weight', 'bert.encoder.layer.21.attention_self.key.bias', 'bert.encoder.layer.21.attention_self.value.weight', 'bert.encoder.layer.21.attention_self.value.bias', 'bert.encoder.layer.21.attention_self.v_query.weight', 'bert.encoder.layer.21.attention_self.v_query.bias', 'bert.encoder.layer.21.attention_self.v_key.weight', 'bert.encoder.layer.21.attention_self.v_key.bias', 'bert.encoder.layer.21.attention_self.v_value.weight', 'bert.encoder.layer.21.attention_self.v_value.bias', 'bert.encoder.layer.21.attention_output.dense.weight', 'bert.encoder.layer.21.attention_output.dense.bias', 'bert.encoder.layer.21.attention_output.LayerNorm.weight', 'bert.encoder.layer.21.attention_output.LayerNorm.bias', 'bert.encoder.layer.21.attention_output.v_dense.weight', 'bert.encoder.layer.21.attention_output.v_dense.bias', 'bert.encoder.layer.21.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.21.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.22.attention_self.v_query.weight', 'bert.encoder.layer.22.attention_self.v_query.bias', 'bert.encoder.layer.22.attention_self.v_key.weight', 'bert.encoder.layer.22.attention_self.v_key.bias', 'bert.encoder.layer.22.attention_self.v_value.weight', 'bert.encoder.layer.22.attention_self.v_value.bias', 'bert.encoder.layer.22.attention_output.v_dense.weight', 'bert.encoder.layer.22.attention_output.v_dense.bias', 'bert.encoder.layer.22.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.22.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.23.intermediate.v_dense.weight', 'bert.encoder.layer.23.intermediate.v_dense.bias', 'bert.encoder.layer.23.output.v_dense.weight', 'bert.encoder.layer.23.output.v_dense.bias', 'bert.encoder.layer.23.output.v_LayerNorm.weight', 'bert.encoder.layer.23.output.v_LayerNorm.bias', 'bert.encoder.layer.24.attention_self.query.weight', 'bert.encoder.layer.24.attention_self.query.bias', 'bert.encoder.layer.24.attention_self.key.weight', 'bert.encoder.layer.24.attention_self.key.bias', 'bert.encoder.layer.24.attention_self.value.weight', 'bert.encoder.layer.24.attention_self.value.bias', 'bert.encoder.layer.24.attention_self.v_query.weight', 'bert.encoder.layer.24.attention_self.v_query.bias', 'bert.encoder.layer.24.attention_self.v_key.weight', 'bert.encoder.layer.24.attention_self.v_key.bias', 'bert.encoder.layer.24.attention_self.v_value.weight', 'bert.encoder.layer.24.attention_self.v_value.bias', 'bert.encoder.layer.24.attention_output.dense.weight', 'bert.encoder.layer.24.attention_output.dense.bias', 'bert.encoder.layer.24.attention_output.LayerNorm.weight', 'bert.encoder.layer.24.attention_output.LayerNorm.bias', 'bert.encoder.layer.24.attention_output.v_dense.weight', 'bert.encoder.layer.24.attention_output.v_dense.bias', 'bert.encoder.layer.24.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.24.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.25.attention_self.v_query.weight', 'bert.encoder.layer.25.attention_self.v_query.bias', 'bert.encoder.layer.25.attention_self.v_key.weight', 'bert.encoder.layer.25.attention_self.v_key.bias', 'bert.encoder.layer.25.attention_self.v_value.weight', 'bert.encoder.layer.25.attention_self.v_value.bias', 'bert.encoder.layer.25.attention_output.v_dense.weight', 'bert.encoder.layer.25.attention_output.v_dense.bias', 'bert.encoder.layer.25.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.25.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.26.intermediate.v_dense.weight', 'bert.encoder.layer.26.intermediate.v_dense.bias', 'bert.encoder.layer.26.output.v_dense.weight', 'bert.encoder.layer.26.output.v_dense.bias', 'bert.encoder.layer.26.output.v_LayerNorm.weight', 'bert.encoder.layer.26.output.v_LayerNorm.bias', 'bert.encoder.layer.27.attention_self.query.weight', 'bert.encoder.layer.27.attention_self.query.bias', 'bert.encoder.layer.27.attention_self.key.weight', 'bert.encoder.layer.27.attention_self.key.bias', 'bert.encoder.layer.27.attention_self.value.weight', 'bert.encoder.layer.27.attention_self.value.bias', 'bert.encoder.layer.27.attention_self.v_query.weight', 'bert.encoder.layer.27.attention_self.v_query.bias', 'bert.encoder.layer.27.attention_self.v_key.weight', 'bert.encoder.layer.27.attention_self.v_key.bias', 'bert.encoder.layer.27.attention_self.v_value.weight', 'bert.encoder.layer.27.attention_self.v_value.bias', 'bert.encoder.layer.27.attention_output.dense.weight', 'bert.encoder.layer.27.attention_output.dense.bias', 'bert.encoder.layer.27.attention_output.LayerNorm.weight', 'bert.encoder.layer.27.attention_output.LayerNorm.bias', 'bert.encoder.layer.27.attention_output.v_dense.weight', 'bert.encoder.layer.27.attention_output.v_dense.bias', 'bert.encoder.layer.27.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.27.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.28.attention_self.query.weight', 'bert.encoder.layer.28.attention_self.query.bias', 'bert.encoder.layer.28.attention_self.key.weight', 'bert.encoder.layer.28.attention_self.key.bias', 'bert.encoder.layer.28.attention_self.value.weight', 'bert.encoder.layer.28.attention_self.value.bias', 'bert.encoder.layer.28.attention_self.v_query.weight', 'bert.encoder.layer.28.attention_self.v_query.bias', 'bert.encoder.layer.28.attention_self.v_key.weight', 'bert.encoder.layer.28.attention_self.v_key.bias', 'bert.encoder.layer.28.attention_self.v_value.weight', 'bert.encoder.layer.28.attention_self.v_value.bias', 'bert.encoder.layer.28.attention_output.dense.weight', 'bert.encoder.layer.28.attention_output.dense.bias', 'bert.encoder.layer.28.attention_output.LayerNorm.weight', 'bert.encoder.layer.28.attention_output.LayerNorm.bias', 'bert.encoder.layer.28.attention_output.v_dense.weight', 'bert.encoder.layer.28.attention_output.v_dense.bias', 'bert.encoder.layer.28.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.28.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.29.intermediate.dense.weight', 'bert.encoder.layer.29.intermediate.dense.bias', 'bert.encoder.layer.29.intermediate.v_dense.weight', 'bert.encoder.layer.29.intermediate.v_dense.bias', 'bert.encoder.layer.29.output.dense.weight', 'bert.encoder.layer.29.output.dense.bias', 'bert.encoder.layer.29.output.LayerNorm.weight', 'bert.encoder.layer.29.output.LayerNorm.bias', 'bert.encoder.layer.29.output.v_dense.weight', 'bert.encoder.layer.29.output.v_dense.bias', 'bert.encoder.layer.29.output.v_LayerNorm.weight', 'bert.encoder.layer.29.output.v_LayerNorm.bias', 'bert.encoder.layer.30.attention_self.query.weight', 'bert.encoder.layer.30.attention_self.query.bias', 'bert.encoder.layer.30.attention_self.key.weight', 'bert.encoder.layer.30.attention_self.key.bias', 'bert.encoder.layer.30.attention_self.value.weight', 'bert.encoder.layer.30.attention_self.value.bias', 'bert.encoder.layer.30.attention_self.v_query.weight', 'bert.encoder.layer.30.attention_self.v_query.bias', 'bert.encoder.layer.30.attention_self.v_key.weight', 'bert.encoder.layer.30.attention_self.v_key.bias', 'bert.encoder.layer.30.attention_self.v_value.weight', 'bert.encoder.layer.30.attention_self.v_value.bias', 'bert.encoder.layer.30.attention_output.dense.weight', 'bert.encoder.layer.30.attention_output.dense.bias', 'bert.encoder.layer.30.attention_output.LayerNorm.weight', 'bert.encoder.layer.30.attention_output.LayerNorm.bias', 'bert.encoder.layer.30.attention_output.v_dense.weight', 'bert.encoder.layer.30.attention_output.v_dense.bias', 'bert.encoder.layer.30.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.30.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.31.attention_self.query.weight', 'bert.encoder.layer.31.attention_self.query.bias', 'bert.encoder.layer.31.attention_self.key.weight', 'bert.encoder.layer.31.attention_self.key.bias', 'bert.encoder.layer.31.attention_self.value.weight', 'bert.encoder.layer.31.attention_self.value.bias', 'bert.encoder.layer.31.attention_self.v_query.weight', 'bert.encoder.layer.31.attention_self.v_query.bias', 'bert.encoder.layer.31.attention_self.v_key.weight', 'bert.encoder.layer.31.attention_self.v_key.bias', 'bert.encoder.layer.31.attention_self.v_value.weight', 'bert.encoder.layer.31.attention_self.v_value.bias', 'bert.encoder.layer.31.attention_output.dense.weight', 'bert.encoder.layer.31.attention_output.dense.bias', 'bert.encoder.layer.31.attention_output.LayerNorm.weight', 'bert.encoder.layer.31.attention_output.LayerNorm.bias', 'bert.encoder.layer.31.attention_output.v_dense.weight', 'bert.encoder.layer.31.attention_output.v_dense.bias', 'bert.encoder.layer.31.attention_output.v_LayerNorm.weight', 'bert.encoder.layer.31.attention_output.v_LayerNorm.bias', 'bert.encoder.layer.32.intermediate.dense.weight', 'bert.encoder.layer.32.intermediate.dense.bias', 'bert.encoder.layer.32.intermediate.v_dense.weight', 'bert.encoder.layer.32.intermediate.v_dense.bias', 'bert.encoder.layer.32.output.dense.weight', 'bert.encoder.layer.32.output.dense.bias', 'bert.encoder.layer.32.output.LayerNorm.weight', 'bert.encoder.layer.32.output.LayerNorm.bias', 'bert.encoder.layer.32.output.v_dense.weight', 'bert.encoder.layer.32.output.v_dense.bias', 'bert.encoder.layer.32.output.v_LayerNorm.weight', 'bert.encoder.layer.32.output.v_LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias', 'cls.bi_seq_relationship.weight', 'cls.bi_seq_relationship.bias', 'cls.imagePredictions.transform.dense.weight', 'cls.imagePredictions.transform.dense.bias', 'cls.imagePredictions.transform.LayerNorm.weight', 'cls.imagePredictions.transform.LayerNorm.bias', 'cls.imagePredictions.decoder_dict.0.weight', 'cls.imagePredictions.decoder_dict.0.bias']
12/07/2020 09:07:54 - INFO - volta.utils -   Weights from pretrained model not used in BertForVLPreTraining: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
12/07/2020 09:07:57 - INFO - __main__ -   ** ** * Saving model * ** ** 
12/07/2020 09:07:59 - INFO - __main__ -   >> Trainable Parameters:
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |Name                                                      |Dtype            |Shape          |#Params    |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.bert.t_pooler.dense.weight                         |torch.float32    |(1024, 768)    |786432     |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.bert.t_pooler.dense.bias                           |torch.float32    |(1024,)        |1024       |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.bert.v_pooler.dense.weight                         |torch.float32    |(1024, 768)    |786432     |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.bert.v_pooler.dense.bias                           |torch.float32    |(1024,)        |1024       |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.predictions.bias                               |torch.float32    |(30522,)       |30522      |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.predictions.transform.dense.weight             |torch.float32    |(768, 768)     |589824     |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.predictions.transform.dense.bias               |torch.float32    |(768,)         |768        |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.predictions.transform.LayerNorm.weight         |torch.float32    |(768,)         |768        |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.predictions.transform.LayerNorm.bias           |torch.float32    |(768,)         |768        |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.bi_seq_relationship.weight                     |torch.float32    |(2, 1024)      |2048       |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.bi_seq_relationship.bias                       |torch.float32    |(2,)           |2          |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.imagePredictions.transform.dense.weight        |torch.float32    |(768, 768)     |589824     |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.imagePredictions.transform.dense.bias          |torch.float32    |(768,)         |768        |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.imagePredictions.transform.LayerNorm.weight    |torch.float32    |(768,)         |768        |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.imagePredictions.transform.LayerNorm.bias      |torch.float32    |(768,)         |768        |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.imagePredictions.decoder_dict.0.weight         |torch.float32    |(1601, 768)    |1229568    |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   |module.cls.imagePredictions.decoder_dict.0.bias           |torch.float32    |(1601,)        |1601       |
12/07/2020 09:07:59 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------
12/07/2020 09:07:59 - INFO - __main__ -   >> # TrainableParams:       	4.02	M
12/07/2020 09:07:59 - INFO - __main__ -   >> # NonTrainableParams:    	207.35	M
12/07/2020 09:07:59 - INFO - __main__ -   >> # TotalParams:           	211.37	M
12/07/2020 09:07:59 - INFO - __main__ -   ***** Running training *****
12/07/2020 09:07:59 - INFO - __main__ -     Num examples = 2777649
12/07/2020 09:07:59 - INFO - __main__ -     Batch size = 256
12/07/2020 09:07:59 - INFO - __main__ -     Num steps = 108500
12/07/2020 09:08:19 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 21 Ep: 0.00 masked_t 8.652 masked_v 5.396 NSP 0.704 lr 1.01382e-07
12/07/2020 09:08:31 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 41 Ep: 0.00 masked_t 8.715 masked_v 5.386 NSP 0.707 lr 2.90323e-07
12/07/2020 09:08:43 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 61 Ep: 0.01 masked_t 8.653 masked_v 5.366 NSP 0.701 lr 4.74654e-07
12/07/2020 09:08:56 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 81 Ep: 0.01 masked_t 8.710 masked_v 5.340 NSP 0.700 lr 6.58986e-07
12/07/2020 09:09:08 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 101 Ep: 0.01 masked_t 8.669 masked_v 5.313 NSP 0.702 lr 8.43318e-07
12/07/2020 09:09:20 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 121 Ep: 0.01 masked_t 8.611 masked_v 5.265 NSP 0.699 lr 1.02765e-06
12/07/2020 09:09:32 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 141 Ep: 0.01 masked_t 8.613 masked_v 5.213 NSP 0.699 lr 1.21198e-06
12/07/2020 09:09:45 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 161 Ep: 0.01 masked_t 8.652 masked_v 5.143 NSP 0.700 lr 1.39631e-06
12/07/2020 09:09:57 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 181 Ep: 0.02 masked_t 8.437 masked_v 5.084 NSP 0.700 lr 1.58065e-06
12/07/2020 09:10:09 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 201 Ep: 0.02 masked_t 8.379 masked_v 4.981 NSP 0.700 lr 1.76498e-06
12/07/2020 09:10:22 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 221 Ep: 0.02 masked_t 8.358 masked_v 4.910 NSP 0.700 lr 1.94931e-06
12/07/2020 09:10:34 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 241 Ep: 0.02 masked_t 8.256 masked_v 4.795 NSP 0.701 lr 2.13364e-06
12/07/2020 09:10:47 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 261 Ep: 0.02 masked_t 8.182 masked_v 4.672 NSP 0.701 lr 2.31797e-06
12/07/2020 09:10:59 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 281 Ep: 0.03 masked_t 8.137 masked_v 4.571 NSP 0.699 lr 2.5023e-06
12/07/2020 09:11:11 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 301 Ep: 0.03 masked_t 8.126 masked_v 4.465 NSP 0.702 lr 2.68664e-06
12/07/2020 09:11:23 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 321 Ep: 0.03 masked_t 7.973 masked_v 4.328 NSP 0.695 lr 2.87097e-06
12/07/2020 09:11:35 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 341 Ep: 0.03 masked_t 7.984 masked_v 4.192 NSP 0.698 lr 3.0553e-06
12/07/2020 09:11:47 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 361 Ep: 0.03 masked_t 7.918 masked_v 4.127 NSP 0.699 lr 3.23963e-06
12/07/2020 09:11:59 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 381 Ep: 0.04 masked_t 7.867 masked_v 3.988 NSP 0.697 lr 3.42396e-06
12/07/2020 09:12:11 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 401 Ep: 0.04 masked_t 7.858 masked_v 3.924 NSP 0.698 lr 3.60829e-06
12/07/2020 09:12:24 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 421 Ep: 0.04 masked_t 7.714 masked_v 3.841 NSP 0.701 lr 3.79263e-06
12/07/2020 09:12:36 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 441 Ep: 0.04 masked_t 7.635 masked_v 3.790 NSP 0.698 lr 3.97696e-06
12/07/2020 09:12:48 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 461 Ep: 0.04 masked_t 7.540 masked_v 3.738 NSP 0.697 lr 4.16129e-06
12/07/2020 09:13:01 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 481 Ep: 0.04 masked_t 7.447 masked_v 3.620 NSP 0.696 lr 4.34562e-06
12/07/2020 09:13:14 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 501 Ep: 0.05 masked_t 7.333 masked_v 3.591 NSP 0.697 lr 4.52995e-06
12/07/2020 09:13:27 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 521 Ep: 0.05 masked_t 7.343 masked_v 3.522 NSP 0.695 lr 4.71429e-06
12/07/2020 09:13:39 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 541 Ep: 0.05 masked_t 7.250 masked_v 3.455 NSP 0.700 lr 4.89862e-06
12/07/2020 09:13:52 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 561 Ep: 0.05 masked_t 7.384 masked_v 3.379 NSP 0.697 lr 5.08295e-06
12/07/2020 09:14:04 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 581 Ep: 0.05 masked_t 7.245 masked_v 3.322 NSP 0.696 lr 5.26728e-06
12/07/2020 09:14:16 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 601 Ep: 0.06 masked_t 7.274 masked_v 3.277 NSP 0.696 lr 5.45161e-06
12/07/2020 09:14:28 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 621 Ep: 0.06 masked_t 7.272 masked_v 3.241 NSP 0.696 lr 5.63594e-06
12/07/2020 09:14:41 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 641 Ep: 0.06 masked_t 7.231 masked_v 3.149 NSP 0.695 lr 5.82028e-06
12/07/2020 09:14:53 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 661 Ep: 0.06 masked_t 7.137 masked_v 3.134 NSP 0.696 lr 6.00461e-06
12/07/2020 09:15:06 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 681 Ep: 0.06 masked_t 7.048 masked_v 3.109 NSP 0.694 lr 6.18894e-06
12/07/2020 09:15:18 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 701 Ep: 0.06 masked_t 6.996 masked_v 3.068 NSP 0.695 lr 6.37327e-06
12/07/2020 09:15:30 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 721 Ep: 0.07 masked_t 7.008 masked_v 3.046 NSP 0.696 lr 6.5576e-06
12/07/2020 09:15:43 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 741 Ep: 0.07 masked_t 6.986 masked_v 2.982 NSP 0.696 lr 6.74194e-06
12/07/2020 09:15:55 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 761 Ep: 0.07 masked_t 6.873 masked_v 3.003 NSP 0.695 lr 6.92627e-06
12/07/2020 09:16:07 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 781 Ep: 0.07 masked_t 6.972 masked_v 2.982 NSP 0.694 lr 7.1106e-06
12/07/2020 09:16:20 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 801 Ep: 0.07 masked_t 6.926 masked_v 2.970 NSP 0.692 lr 7.29493e-06
12/07/2020 09:16:32 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 821 Ep: 0.08 masked_t 6.895 masked_v 2.941 NSP 0.695 lr 7.47926e-06
12/07/2020 09:16:45 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 841 Ep: 0.08 masked_t 6.956 masked_v 2.942 NSP 0.693 lr 7.66359e-06
12/07/2020 09:16:57 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 861 Ep: 0.08 masked_t 6.732 masked_v 2.898 NSP 0.694 lr 7.84793e-06
12/07/2020 09:17:09 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 881 Ep: 0.08 masked_t 6.822 masked_v 2.862 NSP 0.693 lr 8.03226e-06
12/07/2020 09:17:22 - INFO - volta.train_utils -   [Conceptual_Caption]: iter 901 Ep: 0.08 masked_t 6.819 masked_v 2.858 NSP 0.694 lr 8.21659e-06
