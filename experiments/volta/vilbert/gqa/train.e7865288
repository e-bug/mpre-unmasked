/gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/tensorpack/callbacks/hooks.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

WARNING:tensorflow:From /gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/tensorpack/tfutils/optimizer.py:18: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/tensorpack/tfutils/sesscreate.py:20: The name tf.train.SessionCreator is deprecated. Please use tf.compat.v1.train.SessionCreator instead.


Bad key "text.kerning_factor" on line 4 in
/gs/hs0/tgb-deepmt/bugliarello.e/envs/volta/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template
or from the matplotlib source distribution
09/12/2020 16:51:23 - INFO - __main__ -   device: cuda n_gpu: 4, distributed training: False, 16-bits training: False
09/12/2020 16:51:24 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/4/19ITA380/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/12/2020 16:51:24 - INFO - volta.task_utils -   Loading GQA Dataset with batch size 256
09/12/2020 16:51:25 - INFO - volta.datasets.gqa_dataset -   Loading from /gs/hs0/tgb-deepmt/bugliarello.e/data/gqa/cache/GQA_train_26.pkl
09/12/2020 16:56:17 - INFO - volta.datasets.gqa_dataset -   Loading from /gs/hs0/tgb-deepmt/bugliarello.e/data/gqa/cache/GQA_val_26.pkl
09/12/2020 16:57:01 - INFO - volta.utils -   logging file at: ../../logs/volta/gqa/GQA_vilbert_base
09/12/2020 16:57:02 - INFO - volta.utils -   loading weights file /gs/hs0/tgb-deepmt/bugliarello.e/checkpoints/conceptual_captions/volta/vilbert/vilbert_base/pytorch_model_9.bin
09/12/2020 16:57:08 - INFO - volta.utils -   
09/12/2020 16:57:08 - INFO - volta.utils -   Weights of BertForVLTasks not initialized from pretrained model: ['clfs_dict.TASK15.logit_fc.0.weight', 'clfs_dict.TASK15.logit_fc.0.bias', 'clfs_dict.TASK15.logit_fc.2.weight', 'clfs_dict.TASK15.logit_fc.2.bias', 'clfs_dict.TASK15.logit_fc.3.weight', 'clfs_dict.TASK15.logit_fc.3.bias']
09/12/2020 16:57:08 - INFO - volta.utils -   Weights from pretrained model not used in BertForVLTasks: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.bi_seq_relationship.weight', 'cls.bi_seq_relationship.bias', 'cls.imagePredictions.transform.dense.weight', 'cls.imagePredictions.transform.dense.bias', 'cls.imagePredictions.transform.LayerNorm.weight', 'cls.imagePredictions.transform.LayerNorm.bias', 'cls.imagePredictions.decoder_dict.0.weight', 'cls.imagePredictions.decoder_dict.0.bias']
09/12/2020 16:57:13 - INFO - __main__ -   >> Trainable Parameters:
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |Name                                                                |Dtype            |Shape           |#Params     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.embeddings.word_embeddings.weight                       |torch.float32    |(30522, 768)    |23440896    |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.embeddings.position_embeddings.weight                   |torch.float32    |(512, 768)      |393216      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.embeddings.token_type_embeddings.weight                 |torch.float32    |(2, 768)        |1536        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.embeddings.LayerNorm.weight                             |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.embeddings.LayerNorm.bias                               |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.v_embeddings.image_embeddings.weight                    |torch.float32    |(1024, 2048)    |2097152     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.v_embeddings.image_embeddings.bias                      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.v_embeddings.image_location_embeddings.weight           |torch.float32    |(1024, 5)       |5120        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.v_embeddings.image_location_embeddings.bias             |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.v_embeddings.LayerNorm.weight                           |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.v_embeddings.LayerNorm.bias                             |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.0.attention_self.query.weight             |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.0.attention_self.query.bias               |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.0.attention_self.key.weight               |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.0.attention_self.key.bias                 |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.0.attention_self.value.weight             |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.0.attention_self.value.bias               |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.0.attention_output.dense.weight           |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.0.attention_output.dense.bias             |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.0.attention_output.LayerNorm.weight       |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.0.attention_output.LayerNorm.bias         |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.1.intermediate.dense.weight               |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.1.intermediate.dense.bias                 |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.1.output.dense.weight                     |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.1.output.dense.bias                       |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.1.output.LayerNorm.weight                 |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.1.output.LayerNorm.bias                   |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.2.attention_self.query.weight             |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.2.attention_self.query.bias               |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.2.attention_self.key.weight               |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.2.attention_self.key.bias                 |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.2.attention_self.value.weight             |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.2.attention_self.value.bias               |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.2.attention_output.dense.weight           |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.2.attention_output.dense.bias             |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.2.attention_output.LayerNorm.weight       |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.2.attention_output.LayerNorm.bias         |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.3.intermediate.dense.weight               |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.3.intermediate.dense.bias                 |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.3.output.dense.weight                     |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.3.output.dense.bias                       |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.3.output.LayerNorm.weight                 |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.3.output.LayerNorm.bias                   |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.4.attention_self.query.weight             |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.4.attention_self.query.bias               |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.4.attention_self.key.weight               |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.4.attention_self.key.bias                 |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.4.attention_self.value.weight             |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.4.attention_self.value.bias               |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.4.attention_output.dense.weight           |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.4.attention_output.dense.bias             |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.4.attention_output.LayerNorm.weight       |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.4.attention_output.LayerNorm.bias         |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.5.intermediate.dense.weight               |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.5.intermediate.dense.bias                 |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.5.output.dense.weight                     |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.5.output.dense.bias                       |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.5.output.LayerNorm.weight                 |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.5.output.LayerNorm.bias                   |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.6.attention_self.query.weight             |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.6.attention_self.query.bias               |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.6.attention_self.key.weight               |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.6.attention_self.key.bias                 |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.6.attention_self.value.weight             |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.6.attention_self.value.bias               |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.6.attention_output.dense.weight           |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.6.attention_output.dense.bias             |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.6.attention_output.LayerNorm.weight       |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.6.attention_output.LayerNorm.bias         |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.7.intermediate.dense.weight               |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.7.intermediate.dense.bias                 |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.7.output.dense.weight                     |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.7.output.dense.bias                       |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.7.output.LayerNorm.weight                 |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.7.output.LayerNorm.bias                   |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.8.attention_self.query.weight             |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.8.attention_self.query.bias               |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.8.attention_self.key.weight               |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.8.attention_self.key.bias                 |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.8.attention_self.value.weight             |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.8.attention_self.value.bias               |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.8.attention_output.dense.weight           |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.8.attention_output.dense.bias             |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.8.attention_output.LayerNorm.weight       |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.8.attention_output.LayerNorm.bias         |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.9.intermediate.dense.weight               |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.9.intermediate.dense.bias                 |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.9.output.dense.weight                     |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.9.output.dense.bias                       |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.9.output.LayerNorm.weight                 |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.9.output.LayerNorm.bias                   |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.10.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.10.attention_self.query.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.10.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.10.attention_self.key.bias                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.10.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.10.attention_self.value.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.10.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.10.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.10.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.10.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.11.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.11.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.11.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.11.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.11.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.11.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_self.query.weight            |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_self.query.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_self.key.weight              |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_self.key.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_self.value.weight            |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_self.value.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_self.v_query.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_self.v_query.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_self.v_key.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_self.v_key.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_self.v_value.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_self.v_value.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_output.dense.weight          |torch.float32    |(768, 1024)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_output.v_dense.weight        |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_output.v_dense.bias          |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_output.v_LayerNorm.weight    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.12.attention_output.v_LayerNorm.bias      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.13.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.13.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.13.intermediate.v_dense.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.13.intermediate.v_dense.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.13.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.13.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.13.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.13.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.13.output.v_dense.weight                  |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.13.output.v_dense.bias                    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.13.output.v_LayerNorm.weight              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.13.output.v_LayerNorm.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_self.query.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_self.key.bias                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_self.value.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_self.v_query.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_self.v_query.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_self.v_key.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_self.v_key.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_self.v_value.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_self.v_value.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_output.v_dense.weight        |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_output.v_dense.bias          |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_output.v_LayerNorm.weight    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.14.attention_output.v_LayerNorm.bias      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.15.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.15.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.15.intermediate.v_dense.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.15.intermediate.v_dense.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.15.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.15.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.15.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.15.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.15.output.v_dense.weight                  |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.15.output.v_dense.bias                    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.15.output.v_LayerNorm.weight              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.15.output.v_LayerNorm.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_self.query.weight            |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_self.query.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_self.key.weight              |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_self.key.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_self.value.weight            |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_self.value.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_self.v_query.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_self.v_query.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_self.v_key.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_self.v_key.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_self.v_value.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_self.v_value.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_output.dense.weight          |torch.float32    |(768, 1024)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_output.v_dense.weight        |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_output.v_dense.bias          |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_output.v_LayerNorm.weight    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.16.attention_output.v_LayerNorm.bias      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.17.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.17.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.17.intermediate.v_dense.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.17.intermediate.v_dense.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.17.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.17.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.17.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.17.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.17.output.v_dense.weight                  |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.17.output.v_dense.bias                    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.17.output.v_LayerNorm.weight              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.17.output.v_LayerNorm.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_self.query.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_self.key.bias                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_self.value.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_self.v_query.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_self.v_query.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_self.v_key.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_self.v_key.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_self.v_value.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_self.v_value.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_output.v_dense.weight        |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_output.v_dense.bias          |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_output.v_LayerNorm.weight    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.18.attention_output.v_LayerNorm.bias      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.19.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.19.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.19.intermediate.v_dense.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.19.intermediate.v_dense.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.19.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.19.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.19.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.19.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.19.output.v_dense.weight                  |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.19.output.v_dense.bias                    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.19.output.v_LayerNorm.weight              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.19.output.v_LayerNorm.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_self.query.weight            |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_self.query.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_self.key.weight              |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_self.key.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_self.value.weight            |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_self.value.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_self.v_query.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_self.v_query.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_self.v_key.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_self.v_key.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_self.v_value.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_self.v_value.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_output.dense.weight          |torch.float32    |(768, 1024)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_output.v_dense.weight        |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_output.v_dense.bias          |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_output.v_LayerNorm.weight    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.20.attention_output.v_LayerNorm.bias      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.21.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.21.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.21.intermediate.v_dense.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.21.intermediate.v_dense.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.21.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.21.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.21.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.21.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.21.output.v_dense.weight                  |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.21.output.v_dense.bias                    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.21.output.v_LayerNorm.weight              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.21.output.v_LayerNorm.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_self.query.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_self.key.bias                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_self.value.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_self.v_query.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_self.v_query.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_self.v_key.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_self.v_key.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_self.v_value.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_self.v_value.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_output.v_dense.weight        |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_output.v_dense.bias          |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_output.v_LayerNorm.weight    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.22.attention_output.v_LayerNorm.bias      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.23.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.23.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.23.intermediate.v_dense.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.23.intermediate.v_dense.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.23.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.23.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.23.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.23.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.23.output.v_dense.weight                  |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.23.output.v_dense.bias                    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.23.output.v_LayerNorm.weight              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.23.output.v_LayerNorm.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_self.query.weight            |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_self.query.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_self.key.weight              |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_self.key.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_self.value.weight            |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_self.value.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_self.v_query.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_self.v_query.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_self.v_key.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_self.v_key.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_self.v_value.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_self.v_value.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_output.dense.weight          |torch.float32    |(768, 1024)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_output.v_dense.weight        |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_output.v_dense.bias          |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_output.v_LayerNorm.weight    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.24.attention_output.v_LayerNorm.bias      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.25.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.25.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.25.intermediate.v_dense.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.25.intermediate.v_dense.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.25.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.25.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.25.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.25.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.25.output.v_dense.weight                  |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.25.output.v_dense.bias                    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.25.output.v_LayerNorm.weight              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.25.output.v_LayerNorm.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_self.query.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_self.key.bias                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_self.value.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_self.v_query.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_self.v_query.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_self.v_key.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_self.v_key.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_self.v_value.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_self.v_value.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_output.v_dense.weight        |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_output.v_dense.bias          |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_output.v_LayerNorm.weight    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.26.attention_output.v_LayerNorm.bias      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.27.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.27.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.27.intermediate.v_dense.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.27.intermediate.v_dense.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.27.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.27.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.27.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.27.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.27.output.v_dense.weight                  |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.27.output.v_dense.bias                    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.27.output.v_LayerNorm.weight              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.27.output.v_LayerNorm.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_self.query.weight            |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_self.query.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_self.key.weight              |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_self.key.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_self.value.weight            |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_self.value.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_self.v_query.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_self.v_query.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_self.v_key.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_self.v_key.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_self.v_value.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_self.v_value.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_output.dense.weight          |torch.float32    |(768, 1024)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_output.v_dense.weight        |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_output.v_dense.bias          |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_output.v_LayerNorm.weight    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.28.attention_output.v_LayerNorm.bias      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.29.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.29.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.29.intermediate.v_dense.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.29.intermediate.v_dense.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.29.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.29.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.29.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.29.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.29.output.v_dense.weight                  |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.29.output.v_dense.bias                    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.29.output.v_LayerNorm.weight              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.29.output.v_LayerNorm.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_self.query.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_self.key.bias                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_self.value.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_self.v_query.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_self.v_query.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_self.v_key.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_self.v_key.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_self.v_value.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_self.v_value.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_output.v_dense.weight        |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_output.v_dense.bias          |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_output.v_LayerNorm.weight    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.30.attention_output.v_LayerNorm.bias      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.31.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.31.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.31.intermediate.v_dense.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.31.intermediate.v_dense.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.31.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.31.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.31.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.31.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.31.output.v_dense.weight                  |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.31.output.v_dense.bias                    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.31.output.v_LayerNorm.weight              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.31.output.v_LayerNorm.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_self.query.weight            |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_self.query.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_self.key.weight              |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_self.key.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_self.value.weight            |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_self.value.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_self.v_query.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_self.v_query.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_self.v_key.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_self.v_key.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_self.v_value.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_self.v_value.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_output.dense.weight          |torch.float32    |(768, 1024)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_output.v_dense.weight        |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_output.v_dense.bias          |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_output.v_LayerNorm.weight    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.32.attention_output.v_LayerNorm.bias      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.33.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.33.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.33.intermediate.v_dense.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.33.intermediate.v_dense.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.33.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.33.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.33.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.33.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.33.output.v_dense.weight                  |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.33.output.v_dense.bias                    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.33.output.v_LayerNorm.weight              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.33.output.v_LayerNorm.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_self.query.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_self.key.bias                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_self.value.bias              |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_self.v_query.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_self.v_query.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_self.v_key.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_self.v_key.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_self.v_value.weight          |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_self.v_value.bias            |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_output.dense.bias            |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_output.v_dense.weight        |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_output.v_dense.bias          |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_output.v_LayerNorm.weight    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.34.attention_output.v_LayerNorm.bias      |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.35.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.35.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.35.intermediate.v_dense.weight            |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.35.intermediate.v_dense.bias              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.35.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.35.output.dense.bias                      |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.35.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.35.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.35.output.v_dense.weight                  |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.35.output.v_dense.bias                    |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.35.output.v_LayerNorm.weight              |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.encoder.layer.35.output.v_LayerNorm.bias                |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.t_pooler.dense.weight                                   |torch.float32    |(1024, 768)     |786432      |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.t_pooler.dense.bias                                     |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.v_pooler.dense.weight                                   |torch.float32    |(1024, 1024)    |1048576     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.bert.v_pooler.dense.bias                                     |torch.float32    |(1024,)         |1024        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.clfs_dict.TASK15.logit_fc.0.weight                           |torch.float32    |(1536, 1024)    |1572864     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.clfs_dict.TASK15.logit_fc.0.bias                             |torch.float32    |(1536,)         |1536        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.clfs_dict.TASK15.logit_fc.2.weight                           |torch.float32    |(1536,)         |1536        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.clfs_dict.TASK15.logit_fc.2.bias                             |torch.float32    |(1536,)         |1536        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.clfs_dict.TASK15.logit_fc.3.weight                           |torch.float32    |(1842, 1536)    |2829312     |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   |module.clfs_dict.TASK15.logit_fc.3.bias                             |torch.float32    |(1842,)         |1842        |
09/12/2020 16:57:13 - INFO - __main__ -   ----------------------------------------------------------------------------------------------------------------------
09/12/2020 16:57:13 - INFO - __main__ -   >> # TrainableParams:       	240.11	M
09/12/2020 16:57:13 - INFO - __main__ -   >> # NonTrainableParams:    	0.00	M
09/12/2020 16:57:13 - INFO - __main__ -   >> # TotalParams:           	240.11	M

Epoch:   0%|          | 0/20 [00:00<?, ?it/s]09/12/2020 17:01:19 - INFO - volta.utils -   [GQA]: iter 21 Ep: 0.01 loss 1402.015 score 0.000 lr 5.97177e-08 
09/12/2020 17:02:24 - INFO - volta.utils -   [GQA]: iter 41 Ep: 0.01 loss 1378.619 score 0.000 lr 1.7101e-07 
09/12/2020 17:03:46 - INFO - volta.utils -   [GQA]: iter 61 Ep: 0.02 loss 1336.794 score 0.000 lr 2.79587e-07 
09/12/2020 17:05:42 - INFO - volta.utils -   [GQA]: iter 81 Ep: 0.02 loss 1272.973 score 0.000 lr 3.88165e-07 
09/12/2020 17:07:30 - INFO - volta.utils -   [GQA]: iter 101 Ep: 0.03 loss 1191.115 score 0.000 lr 4.96743e-07 
09/12/2020 17:09:11 - INFO - volta.utils -   [GQA]: iter 121 Ep: 0.03 loss 1104.570 score 0.000 lr 6.0532e-07 
09/12/2020 17:10:49 - INFO - volta.utils -   [GQA]: iter 141 Ep: 0.04 loss 1021.360 score 0.000 lr 7.13898e-07 
09/12/2020 17:13:41 - INFO - volta.utils -   [GQA]: iter 161 Ep: 0.04 loss 945.699 score 0.000 lr 8.22476e-07 
09/12/2020 17:15:00 - INFO - volta.utils -   [GQA]: iter 181 Ep: 0.05 loss 878.253 score 0.000 lr 9.31053e-07 
09/12/2020 17:17:30 - INFO - volta.utils -   [GQA]: iter 201 Ep: 0.05 loss 817.813 score 0.000 lr 1.03963e-06 
09/12/2020 17:18:45 - INFO - volta.utils -   [GQA]: iter 221 Ep: 0.06 loss 761.276 score 0.000 lr 1.14821e-06 
09/12/2020 17:21:22 - INFO - volta.utils -   [GQA]: iter 241 Ep: 0.07 loss 708.050 score 0.000 lr 1.25679e-06 
09/12/2020 17:23:08 - INFO - volta.utils -   [GQA]: iter 261 Ep: 0.07 loss 658.110 score 0.001 lr 1.36536e-06 
09/12/2020 17:24:51 - INFO - volta.utils -   [GQA]: iter 281 Ep: 0.08 loss 611.974 score 0.000 lr 1.47394e-06 
09/12/2020 17:26:30 - INFO - volta.utils -   [GQA]: iter 301 Ep: 0.08 loss 568.276 score 0.000 lr 1.58252e-06 
09/12/2020 17:28:47 - INFO - volta.utils -   [GQA]: iter 321 Ep: 0.09 loss 527.628 score 0.000 lr 1.6911e-06 
09/12/2020 17:30:02 - INFO - volta.utils -   [GQA]: iter 341 Ep: 0.09 loss 489.960 score 0.000 lr 1.79967e-06 
09/12/2020 17:32:44 - INFO - volta.utils -   [GQA]: iter 361 Ep: 0.10 loss 453.946 score 0.000 lr 1.90825e-06 
09/12/2020 17:34:17 - INFO - volta.utils -   [GQA]: iter 381 Ep: 0.10 loss 420.946 score 0.001 lr 2.01683e-06 
09/12/2020 17:35:39 - INFO - volta.utils -   [GQA]: iter 401 Ep: 0.11 loss 390.589 score 0.000 lr 2.12541e-06 
09/12/2020 17:37:25 - INFO - volta.utils -   [GQA]: iter 421 Ep: 0.11 loss 362.319 score 0.000 lr 2.23398e-06 
09/12/2020 17:40:12 - INFO - volta.utils -   [GQA]: iter 441 Ep: 0.12 loss 335.832 score 0.000 lr 2.34256e-06 
09/12/2020 17:41:49 - INFO - volta.utils -   [GQA]: iter 461 Ep: 0.13 loss 311.818 score 0.000 lr 2.45114e-06 
09/12/2020 17:43:20 - INFO - volta.utils -   [GQA]: iter 481 Ep: 0.13 loss 289.323 score 0.001 lr 2.55972e-06 
09/12/2020 17:45:00 - INFO - volta.utils -   [GQA]: iter 501 Ep: 0.14 loss 269.189 score 0.001 lr 2.6683e-06 
09/12/2020 17:47:35 - INFO - volta.utils -   [GQA]: iter 521 Ep: 0.14 loss 250.208 score 0.000 lr 2.77687e-06 
09/12/2020 17:49:09 - INFO - volta.utils -   [GQA]: iter 541 Ep: 0.15 loss 232.629 score 0.001 lr 2.88545e-06 
09/12/2020 17:50:54 - INFO - volta.utils -   [GQA]: iter 561 Ep: 0.15 loss 216.989 score 0.001 lr 2.99403e-06 
09/12/2020 17:52:34 - INFO - volta.utils -   [GQA]: iter 581 Ep: 0.16 loss 202.233 score 0.002 lr 3.10261e-06 
09/12/2020 17:55:38 - INFO - volta.utils -   [GQA]: iter 601 Ep: 0.16 loss 188.734 score 0.002 lr 3.21118e-06 
09/12/2020 17:57:04 - INFO - volta.utils -   [GQA]: iter 621 Ep: 0.17 loss 176.316 score 0.005 lr 3.31976e-06 
09/12/2020 17:58:49 - INFO - volta.utils -   [GQA]: iter 641 Ep: 0.17 loss 165.056 score 0.009 lr 3.42834e-06 
09/12/2020 18:01:04 - INFO - volta.utils -   [GQA]: iter 661 Ep: 0.18 loss 154.466 score 0.014 lr 3.53692e-06 
09/12/2020 18:03:45 - INFO - volta.utils -   [GQA]: iter 681 Ep: 0.18 loss 144.817 score 0.033 lr 3.64549e-06 
09/12/2020 18:05:30 - INFO - volta.utils -   [GQA]: iter 701 Ep: 0.19 loss 135.942 score 0.049 lr 3.75407e-06 
09/12/2020 18:07:07 - INFO - volta.utils -   [GQA]: iter 721 Ep: 0.20 loss 127.775 score 0.062 lr 3.86265e-06 
09/12/2020 18:09:14 - INFO - volta.utils -   [GQA]: iter 741 Ep: 0.20 loss 120.187 score 0.083 lr 3.97123e-06 
09/12/2020 18:12:09 - INFO - volta.utils -   [GQA]: iter 761 Ep: 0.21 loss 113.268 score 0.107 lr 4.0798e-06 
09/12/2020 18:14:00 - INFO - volta.utils -   [GQA]: iter 781 Ep: 0.21 loss 106.779 score 0.134 lr 4.18838e-06 
09/12/2020 18:15:24 - INFO - volta.utils -   [GQA]: iter 801 Ep: 0.22 loss 100.711 score 0.152 lr 4.29696e-06 
09/12/2020 18:17:22 - INFO - volta.utils -   [GQA]: iter 821 Ep: 0.22 loss 95.129 score 0.172 lr 4.40554e-06 
09/12/2020 18:20:15 - INFO - volta.utils -   [GQA]: iter 841 Ep: 0.23 loss 90.053 score 0.160 lr 4.51412e-06 
09/12/2020 18:21:48 - INFO - volta.utils -   [GQA]: iter 861 Ep: 0.23 loss 85.208 score 0.165 lr 4.62269e-06 
09/12/2020 18:23:46 - INFO - volta.utils -   [GQA]: iter 881 Ep: 0.24 loss 80.889 score 0.174 lr 4.73127e-06 
09/12/2020 18:25:24 - INFO - volta.utils -   [GQA]: iter 901 Ep: 0.24 loss 76.736 score 0.174 lr 4.83985e-06 
09/12/2020 18:28:27 - INFO - volta.utils -   [GQA]: iter 921 Ep: 0.25 loss 72.766 score 0.174 lr 4.94843e-06 
09/12/2020 18:30:20 - INFO - volta.utils -   [GQA]: iter 941 Ep: 0.26 loss 69.309 score 0.169 lr 5.057e-06 
09/12/2020 18:32:01 - INFO - volta.utils -   [GQA]: iter 961 Ep: 0.26 loss 65.882 score 0.175 lr 5.16558e-06 
09/12/2020 18:33:43 - INFO - volta.utils -   [GQA]: iter 981 Ep: 0.27 loss 62.745 score 0.187 lr 5.27416e-06 
09/12/2020 18:37:00 - INFO - volta.utils -   [GQA]: iter 1001 Ep: 0.27 loss 59.865 score 0.179 lr 5.38274e-06 
09/12/2020 18:38:29 - INFO - volta.utils -   [GQA]: iter 1021 Ep: 0.28 loss 57.162 score 0.167 lr 5.49131e-06 
09/12/2020 18:40:11 - INFO - volta.utils -   [GQA]: iter 1041 Ep: 0.28 loss 54.557 score 0.177 lr 5.59989e-06 
09/12/2020 18:41:39 - INFO - volta.utils -   [GQA]: iter 1061 Ep: 0.29 loss 52.162 score 0.180 lr 5.70847e-06 
09/12/2020 18:44:23 - INFO - volta.utils -   [GQA]: iter 1081 Ep: 0.29 loss 49.940 score 0.180 lr 5.81705e-06 
09/12/2020 18:46:10 - INFO - volta.utils -   [GQA]: iter 1101 Ep: 0.30 loss 47.772 score 0.177 lr 5.92562e-06 
09/12/2020 18:47:52 - INFO - volta.utils -   [GQA]: iter 1121 Ep: 0.30 loss 45.786 score 0.185 lr 6.0342e-06 
09/12/2020 18:49:02 - INFO - volta.utils -   [GQA]: iter 1141 Ep: 0.31 loss 43.895 score 0.175 lr 6.14278e-06 
09/12/2020 18:51:28 - INFO - volta.utils -   [GQA]: iter 1161 Ep: 0.32 loss 42.175 score 0.167 lr 6.25136e-06 
09/12/2020 18:53:02 - INFO - volta.utils -   [GQA]: iter 1181 Ep: 0.32 loss 40.456 score 0.177 lr 6.35993e-06 
09/12/2020 18:54:24 - INFO - volta.utils -   [GQA]: iter 1201 Ep: 0.33 loss 38.897 score 0.177 lr 6.46851e-06 
09/12/2020 18:56:03 - INFO - volta.utils -   [GQA]: iter 1221 Ep: 0.33 loss 37.466 score 0.175 lr 6.57709e-06 
09/12/2020 18:58:49 - INFO - volta.utils -   [GQA]: iter 1241 Ep: 0.34 loss 36.053 score 0.168 lr 6.68567e-06 
09/12/2020 19:00:10 - INFO - volta.utils -   [GQA]: iter 1261 Ep: 0.34 loss 34.735 score 0.181 lr 6.79425e-06 
09/12/2020 19:01:38 - INFO - volta.utils -   [GQA]: iter 1281 Ep: 0.35 loss 33.444 score 0.172 lr 6.90282e-06 
09/12/2020 19:03:20 - INFO - volta.utils -   [GQA]: iter 1301 Ep: 0.35 loss 32.252 score 0.184 lr 7.0114e-06 
09/12/2020 19:06:10 - INFO - volta.utils -   [GQA]: iter 1321 Ep: 0.36 loss 31.135 score 0.175 lr 7.11998e-06 
09/12/2020 19:07:50 - INFO - volta.utils -   [GQA]: iter 1341 Ep: 0.36 loss 30.059 score 0.186 lr 7.22856e-06 
09/12/2020 19:09:14 - INFO - volta.utils -   [GQA]: iter 1361 Ep: 0.37 loss 29.044 score 0.172 lr 7.33713e-06 
09/12/2020 19:10:27 - INFO - volta.utils -   [GQA]: iter 1381 Ep: 0.37 loss 28.104 score 0.176 lr 7.44571e-06 
09/12/2020 19:13:24 - INFO - volta.utils -   [GQA]: iter 1401 Ep: 0.38 loss 27.158 score 0.176 lr 7.55429e-06 
09/12/2020 19:14:36 - INFO - volta.utils -   [GQA]: iter 1421 Ep: 0.39 loss 26.322 score 0.177 lr 7.66287e-06 
09/12/2020 19:16:21 - INFO - volta.utils -   [GQA]: iter 1441 Ep: 0.39 loss 25.535 score 0.173 lr 7.77144e-06 
09/12/2020 19:17:45 - INFO - volta.utils -   [GQA]: iter 1461 Ep: 0.40 loss 24.724 score 0.175 lr 7.88002e-06 
09/12/2020 19:20:13 - INFO - volta.utils -   [GQA]: iter 1481 Ep: 0.40 loss 24.017 score 0.169 lr 7.9886e-06 
09/12/2020 19:22:00 - INFO - volta.utils -   [GQA]: iter 1501 Ep: 0.41 loss 23.223 score 0.175 lr 8.09718e-06 
09/12/2020 19:23:33 - INFO - volta.utils -   [GQA]: iter 1521 Ep: 0.41 loss 22.607 score 0.178 lr 8.20575e-06 
09/12/2020 19:25:08 - INFO - volta.utils -   [GQA]: iter 1541 Ep: 0.42 loss 21.940 score 0.178 lr 8.31433e-06 
09/12/2020 19:28:33 - INFO - volta.utils -   [GQA]: iter 1561 Ep: 0.42 loss 21.320 score 0.181 lr 8.42291e-06 
09/12/2020 19:30:12 - INFO - volta.utils -   [GQA]: iter 1581 Ep: 0.43 loss 20.738 score 0.179 lr 8.53149e-06 
09/12/2020 19:31:46 - INFO - volta.utils -   [GQA]: iter 1601 Ep: 0.43 loss 20.157 score 0.185 lr 8.64007e-06 
09/12/2020 19:33:09 - INFO - volta.utils -   [GQA]: iter 1621 Ep: 0.44 loss 19.627 score 0.181 lr 8.74864e-06 
09/12/2020 19:36:23 - INFO - volta.utils -   [GQA]: iter 1641 Ep: 0.45 loss 19.106 score 0.183 lr 8.85722e-06 
09/12/2020 19:38:05 - INFO - volta.utils -   [GQA]: iter 1661 Ep: 0.45 loss 18.630 score 0.179 lr 8.9658e-06 
09/12/2020 19:39:35 - INFO - volta.utils -   [GQA]: iter 1681 Ep: 0.46 loss 18.155 score 0.176 lr 9.07438e-06 
09/12/2020 19:40:39 - INFO - volta.utils -   [GQA]: iter 1701 Ep: 0.46 loss 17.719 score 0.185 lr 9.18295e-06 
09/12/2020 19:44:32 - INFO - volta.utils -   [GQA]: iter 1721 Ep: 0.47 loss 17.289 score 0.175 lr 9.29153e-06 
09/12/2020 19:45:58 - INFO - volta.utils -   [GQA]: iter 1741 Ep: 0.47 loss 16.839 score 0.191 lr 9.40011e-06 
09/12/2020 19:47:48 - INFO - volta.utils -   [GQA]: iter 1761 Ep: 0.48 loss 16.488 score 0.172 lr 9.50869e-06 
09/12/2020 19:49:36 - INFO - volta.utils -   [GQA]: iter 1781 Ep: 0.48 loss 16.144 score 0.171 lr 9.61726e-06 
09/12/2020 19:52:26 - INFO - volta.utils -   [GQA]: iter 1801 Ep: 0.49 loss 15.692 score 0.180 lr 9.72584e-06 
09/12/2020 19:54:02 - INFO - volta.utils -   [GQA]: iter 1821 Ep: 0.49 loss 15.418 score 0.175 lr 9.83442e-06 
09/12/2020 19:55:24 - INFO - volta.utils -   [GQA]: iter 1841 Ep: 0.50 loss 15.037 score 0.164 lr 9.943e-06 
09/12/2020 19:57:13 - INFO - volta.utils -   [GQA]: iter 1861 Ep: 0.51 loss 14.719 score 0.178 lr 1.00516e-05 
09/12/2020 19:59:50 - INFO - volta.utils -   [GQA]: iter 1881 Ep: 0.51 loss 14.402 score 0.175 lr 1.01602e-05 
09/12/2020 20:01:31 - INFO - volta.utils -   [GQA]: iter 1901 Ep: 0.52 loss 14.143 score 0.175 lr 1.02687e-05 
09/12/2020 20:02:55 - INFO - volta.utils -   [GQA]: iter 1921 Ep: 0.52 loss 13.811 score 0.181 lr 1.03773e-05 
09/12/2020 20:03:56 - INFO - volta.utils -   [GQA]: iter 1941 Ep: 0.53 loss 13.569 score 0.166 lr 1.04859e-05 
09/12/2020 20:06:41 - INFO - volta.utils -   [GQA]: iter 1961 Ep: 0.53 loss 13.275 score 0.180 lr 1.05945e-05 
09/12/2020 20:07:57 - INFO - volta.utils -   [GQA]: iter 1981 Ep: 0.54 loss 12.991 score 0.182 lr 1.0703e-05 
09/12/2020 20:08:54 - INFO - volta.utils -   [GQA]: iter 2001 Ep: 0.54 loss 12.779 score 0.167 lr 1.08116e-05 
09/12/2020 20:10:44 - INFO - volta.utils -   [GQA]: iter 2021 Ep: 0.55 loss 12.505 score 0.187 lr 1.09202e-05 
09/12/2020 20:12:59 - INFO - volta.utils -   [GQA]: iter 2041 Ep: 0.55 loss 12.261 score 0.184 lr 1.10288e-05 
09/12/2020 20:14:33 - INFO - volta.utils -   [GQA]: iter 2061 Ep: 0.56 loss 12.051 score 0.180 lr 1.11374e-05 
09/12/2020 20:16:09 - INFO - volta.utils -   [GQA]: iter 2081 Ep: 0.56 loss 11.831 score 0.174 lr 1.12459e-05 
09/12/2020 20:18:02 - INFO - volta.utils -   [GQA]: iter 2101 Ep: 0.57 loss 11.608 score 0.187 lr 1.13545e-05 
09/12/2020 20:19:54 - INFO - volta.utils -   [GQA]: iter 2121 Ep: 0.58 loss 11.461 score 0.171 lr 1.14631e-05 
09/12/2020 20:21:44 - INFO - volta.utils -   [GQA]: iter 2141 Ep: 0.58 loss 11.241 score 0.172 lr 1.15717e-05 
09/12/2020 20:23:11 - INFO - volta.utils -   [GQA]: iter 2161 Ep: 0.59 loss 11.051 score 0.176 lr 1.16802e-05 
09/12/2020 20:25:13 - INFO - volta.utils -   [GQA]: iter 2181 Ep: 0.59 loss 10.858 score 0.178 lr 1.17888e-05 
09/12/2020 20:26:45 - INFO - volta.utils -   [GQA]: iter 2201 Ep: 0.60 loss 10.660 score 0.182 lr 1.18974e-05 
09/12/2020 20:28:25 - INFO - volta.utils -   [GQA]: iter 2221 Ep: 0.60 loss 10.469 score 0.187 lr 1.2006e-05 
09/12/2020 20:29:47 - INFO - volta.utils -   [GQA]: iter 2241 Ep: 0.61 loss 10.384 score 0.175 lr 1.21145e-05 
09/12/2020 20:32:01 - INFO - volta.utils -   [GQA]: iter 2261 Ep: 0.61 loss 10.185 score 0.193 lr 1.22231e-05 
09/12/2020 20:33:36 - INFO - volta.utils -   [GQA]: iter 2281 Ep: 0.62 loss 10.023 score 0.193 lr 1.23317e-05 
09/12/2020 20:35:10 - INFO - volta.utils -   [GQA]: iter 2301 Ep: 0.62 loss 9.866 score 0.218 lr 1.24403e-05 
09/12/2020 20:36:38 - INFO - volta.utils -   [GQA]: iter 2321 Ep: 0.63 loss 9.744 score 0.209 lr 1.25489e-05 
09/12/2020 20:38:50 - INFO - volta.utils -   [GQA]: iter 2341 Ep: 0.64 loss 9.531 score 0.228 lr 1.26574e-05 
09/12/2020 20:40:32 - INFO - volta.utils -   [GQA]: iter 2361 Ep: 0.64 loss 9.432 score 0.217 lr 1.2766e-05 
09/12/2020 20:42:04 - INFO - volta.utils -   [GQA]: iter 2381 Ep: 0.65 loss 9.229 score 0.232 lr 1.28746e-05 
09/12/2020 20:43:28 - INFO - volta.utils -   [GQA]: iter 2401 Ep: 0.65 loss 9.121 score 0.227 lr 1.29832e-05 
09/12/2020 20:46:05 - INFO - volta.utils -   [GQA]: iter 2421 Ep: 0.66 loss 9.035 score 0.224 lr 1.30917e-05 
09/12/2020 20:47:17 - INFO - volta.utils -   [GQA]: iter 2441 Ep: 0.66 loss 8.891 score 0.231 lr 1.32003e-05 
09/12/2020 20:48:47 - INFO - volta.utils -   [GQA]: iter 2461 Ep: 0.67 loss 8.835 score 0.225 lr 1.33089e-05 
09/12/2020 20:50:20 - INFO - volta.utils -   [GQA]: iter 2481 Ep: 0.67 loss 8.644 score 0.233 lr 1.34175e-05 
09/12/2020 20:52:23 - INFO - volta.utils -   [GQA]: iter 2501 Ep: 0.68 loss 8.598 score 0.229 lr 1.35261e-05 
09/12/2020 20:53:49 - INFO - volta.utils -   [GQA]: iter 2521 Ep: 0.68 loss 8.489 score 0.226 lr 1.36346e-05 
09/12/2020 20:55:43 - INFO - volta.utils -   [GQA]: iter 2541 Ep: 0.69 loss 8.380 score 0.216 lr 1.37432e-05 
09/12/2020 20:57:12 - INFO - volta.utils -   [GQA]: iter 2561 Ep: 0.70 loss 8.215 score 0.231 lr 1.38518e-05 
09/12/2020 20:59:59 - INFO - volta.utils -   [GQA]: iter 2581 Ep: 0.70 loss 8.104 score 0.233 lr 1.39604e-05 
09/12/2020 21:01:59 - INFO - volta.utils -   [GQA]: iter 2601 Ep: 0.71 loss 8.023 score 0.235 lr 1.40689e-05 
09/12/2020 21:03:57 - INFO - volta.utils -   [GQA]: iter 2621 Ep: 0.71 loss 7.892 score 0.235 lr 1.41775e-05 
09/12/2020 21:05:41 - INFO - volta.utils -   [GQA]: iter 2641 Ep: 0.72 loss 7.836 score 0.230 lr 1.42861e-05 
09/12/2020 21:07:59 - INFO - volta.utils -   [GQA]: iter 2661 Ep: 0.72 loss 7.761 score 0.238 lr 1.43947e-05 
09/12/2020 21:09:36 - INFO - volta.utils -   [GQA]: iter 2681 Ep: 0.73 loss 7.671 score 0.236 lr 1.45033e-05 
09/12/2020 21:11:05 - INFO - volta.utils -   [GQA]: iter 2701 Ep: 0.73 loss 7.573 score 0.234 lr 1.46118e-05 
09/12/2020 21:12:25 - INFO - volta.utils -   [GQA]: iter 2721 Ep: 0.74 loss 7.529 score 0.228 lr 1.47204e-05 
09/12/2020 21:15:55 - INFO - volta.utils -   [GQA]: iter 2741 Ep: 0.74 loss 7.455 score 0.230 lr 1.4829e-05 
09/12/2020 21:17:57 - INFO - volta.utils -   [GQA]: iter 2761 Ep: 0.75 loss 7.336 score 0.227 lr 1.49376e-05 
09/12/2020 21:20:16 - INFO - volta.utils -   [GQA]: iter 2781 Ep: 0.75 loss 7.258 score 0.238 lr 1.50461e-05 
09/12/2020 21:22:01 - INFO - volta.utils -   [GQA]: iter 2801 Ep: 0.76 loss 7.211 score 0.235 lr 1.51547e-05 
09/12/2020 21:23:55 - INFO - volta.utils -   [GQA]: iter 2821 Ep: 0.77 loss 7.165 score 0.229 lr 1.52633e-05 
09/12/2020 21:25:09 - INFO - volta.utils -   [GQA]: iter 2841 Ep: 0.77 loss 7.128 score 0.224 lr 1.53719e-05 
09/12/2020 21:27:53 - INFO - volta.utils -   [GQA]: iter 2861 Ep: 0.78 loss 7.023 score 0.237 lr 1.54805e-05 
09/12/2020 21:29:25 - INFO - volta.utils -   [GQA]: iter 2881 Ep: 0.78 loss 6.986 score 0.217 lr 1.5589e-05 
09/12/2020 21:31:23 - INFO - volta.utils -   [GQA]: iter 2901 Ep: 0.79 loss 6.990 score 0.221 lr 1.56976e-05 
09/12/2020 21:33:07 - INFO - volta.utils -   [GQA]: iter 2921 Ep: 0.79 loss 6.846 score 0.231 lr 1.58062e-05 
09/12/2020 21:37:17 - INFO - volta.utils -   [GQA]: iter 2941 Ep: 0.80 loss 6.780 score 0.225 lr 1.59148e-05 
09/12/2020 21:39:35 - INFO - volta.utils -   [GQA]: iter 2961 Ep: 0.80 loss 6.705 score 0.227 lr 1.60233e-05 
09/12/2020 21:42:10 - INFO - volta.utils -   [GQA]: iter 2981 Ep: 0.81 loss 6.685 score 0.224 lr 1.61319e-05 
09/12/2020 21:43:35 - INFO - volta.utils -   [GQA]: iter 3001 Ep: 0.81 loss 6.615 score 0.234 lr 1.62405e-05 
09/12/2020 21:44:59 - INFO - volta.utils -   [GQA]: iter 3021 Ep: 0.82 loss 6.568 score 0.227 lr 1.63491e-05 
09/12/2020 21:46:09 - INFO - volta.utils -   [GQA]: iter 3041 Ep: 0.83 loss 6.499 score 0.237 lr 1.64577e-05 
09/12/2020 21:48:55 - INFO - volta.utils -   [GQA]: iter 3061 Ep: 0.83 loss 6.474 score 0.236 lr 1.65662e-05 
09/12/2020 21:50:14 - INFO - volta.utils -   [GQA]: iter 3081 Ep: 0.84 loss 6.401 score 0.240 lr 1.66748e-05 
09/12/2020 21:51:21 - INFO - volta.utils -   [GQA]: iter 3101 Ep: 0.84 loss 6.418 score 0.235 lr 1.67834e-05 
09/12/2020 21:52:57 - INFO - volta.utils -   [GQA]: iter 3121 Ep: 0.85 loss 6.318 score 0.235 lr 1.6892e-05 
09/12/2020 21:55:12 - INFO - volta.utils -   [GQA]: iter 3141 Ep: 0.85 loss 6.283 score 0.246 lr 1.70005e-05 
09/12/2020 21:56:45 - INFO - volta.utils -   [GQA]: iter 3161 Ep: 0.86 loss 6.276 score 0.233 lr 1.71091e-05 
09/12/2020 21:57:50 - INFO - volta.utils -   [GQA]: iter 3181 Ep: 0.86 loss 6.164 score 0.245 lr 1.72177e-05 
09/12/2020 21:59:30 - INFO - volta.utils -   [GQA]: iter 3201 Ep: 0.87 loss 6.176 score 0.251 lr 1.73263e-05 
09/12/2020 22:02:42 - INFO - volta.utils -   [GQA]: iter 3221 Ep: 0.87 loss 6.187 score 0.245 lr 1.74349e-05 
09/12/2020 22:04:25 - INFO - volta.utils -   [GQA]: iter 3241 Ep: 0.88 loss 6.079 score 0.240 lr 1.75434e-05 
09/12/2020 22:05:59 - INFO - volta.utils -   [GQA]: iter 3261 Ep: 0.89 loss 6.073 score 0.253 lr 1.7652e-05 
09/12/2020 22:07:20 - INFO - volta.utils -   [GQA]: iter 3281 Ep: 0.89 loss 5.963 score 0.263 lr 1.77606e-05 
09/12/2020 22:09:21 - INFO - volta.utils -   [GQA]: iter 3301 Ep: 0.90 loss 5.965 score 0.253 lr 1.78692e-05 
09/12/2020 22:10:16 - INFO - volta.utils -   [GQA]: iter 3321 Ep: 0.90 loss 5.922 score 0.259 lr 1.79777e-05 
09/12/2020 22:11:18 - INFO - volta.utils -   [GQA]: iter 3341 Ep: 0.91 loss 5.861 score 0.264 lr 1.80863e-05 
09/12/2020 22:13:25 - INFO - volta.utils -   [GQA]: iter 3361 Ep: 0.91 loss 5.879 score 0.261 lr 1.81949e-05 
09/12/2020 22:16:06 - INFO - volta.utils -   [GQA]: iter 3381 Ep: 0.92 loss 5.871 score 0.250 lr 1.83035e-05 
09/12/2020 22:17:43 - INFO - volta.utils -   [GQA]: iter 3401 Ep: 0.92 loss 5.757 score 0.260 lr 1.84121e-05 
09/12/2020 22:19:25 - INFO - volta.utils -   [GQA]: iter 3421 Ep: 0.93 loss 5.693 score 0.263 lr 1.85206e-05 
09/12/2020 22:21:30 - INFO - volta.utils -   [GQA]: iter 3441 Ep: 0.93 loss 5.669 score 0.267 lr 1.86292e-05 
09/12/2020 22:23:58 - INFO - volta.utils -   [GQA]: iter 3461 Ep: 0.94 loss 5.632 score 0.280 lr 1.87378e-05 
09/12/2020 22:25:46 - INFO - volta.utils -   [GQA]: iter 3481 Ep: 0.94 loss 5.640 score 0.273 lr 1.88464e-05 
09/12/2020 22:27:02 - INFO - volta.utils -   [GQA]: iter 3501 Ep: 0.95 loss 5.577 score 0.271 lr 1.89549e-05 
09/12/2020 22:28:21 - INFO - volta.utils -   [GQA]: iter 3521 Ep: 0.96 loss 5.531 score 0.279 lr 1.90635e-05 
09/12/2020 22:30:11 - INFO - volta.utils -   [GQA]: iter 3541 Ep: 0.96 loss 5.548 score 0.266 lr 1.91721e-05 
09/12/2020 22:32:19 - INFO - volta.utils -   [GQA]: iter 3561 Ep: 0.97 loss 5.561 score 0.266 lr 1.92807e-05 
09/12/2020 22:33:55 - INFO - volta.utils -   [GQA]: iter 3581 Ep: 0.97 loss 5.328 score 0.286 lr 1.93893e-05 
09/12/2020 22:34:58 - INFO - volta.utils -   [GQA]: iter 3601 Ep: 0.98 loss 5.514 score 0.263 lr 1.94978e-05 
09/12/2020 22:37:44 - INFO - volta.utils -   [GQA]: iter 3621 Ep: 0.98 loss 5.426 score 0.268 lr 1.96064e-05 
09/12/2020 22:39:40 - INFO - volta.utils -   [GQA]: iter 3641 Ep: 0.99 loss 5.315 score 0.275 lr 1.9715e-05 
09/12/2020 22:41:07 - INFO - volta.utils -   [GQA]: iter 3661 Ep: 0.99 loss 5.291 score 0.286 lr 1.98236e-05 
09/12/2020 22:41:59 - INFO - volta.utils -   [GQA]: iter 3681 Ep: 1.00 loss 5.362 score 0.270 lr 1.99321e-05 
09/12/2020 22:42:01 - INFO - __main__ -   ** ** * Saving fine - tuned model ** ** * 

Epoch:   5%|▌         | 1/20 [5:44:54<109:13:17, 20694.60s/it]09/12/2020 23:01:40 - INFO - volta.utils -   Eval task TASK15 on iteration 3685 
09/12/2020 23:01:40 - INFO - volta.utils -   Validation [GQA]: loss 5.168 score 27.745 
09/12/2020 23:01:55 - INFO - volta.utils -   [GQA]: iter 3705 Ep: 1.01 loss 5.298 score 0.272 lr 2.00516e-05 
09/12/2020 23:03:38 - INFO - volta.utils -   [GQA]: iter 3725 Ep: 1.01 loss 5.238 score 0.273 lr 2.0171e-05 
09/12/2020 23:04:54 - INFO - volta.utils -   [GQA]: iter 3745 Ep: 1.02 loss 5.182 score 0.282 lr 2.02796e-05 
09/12/2020 23:06:08 - INFO - volta.utils -   [GQA]: iter 3765 Ep: 1.02 loss 5.189 score 0.286 lr 2.03882e-05 
09/12/2020 23:08:07 - INFO - volta.utils -   [GQA]: iter 3785 Ep: 1.03 loss 5.190 score 0.282 lr 2.04967e-05 
09/12/2020 23:09:34 - INFO - volta.utils -   [GQA]: iter 3805 Ep: 1.03 loss 5.173 score 0.272 lr 2.06053e-05 
09/12/2020 23:10:32 - INFO - volta.utils -   [GQA]: iter 3825 Ep: 1.04 loss 5.115 score 0.291 lr 2.07139e-05 
09/12/2020 23:11:50 - INFO - volta.utils -   [GQA]: iter 3845 Ep: 1.04 loss 5.124 score 0.279 lr 2.08225e-05 
09/12/2020 23:14:27 - INFO - volta.utils -   [GQA]: iter 3865 Ep: 1.05 loss 5.106 score 0.287 lr 2.09311e-05 
09/12/2020 23:15:58 - INFO - volta.utils -   [GQA]: iter 3885 Ep: 1.05 loss 5.048 score 0.286 lr 2.10396e-05 
09/12/2020 23:17:03 - INFO - volta.utils -   [GQA]: iter 3905 Ep: 1.06 loss 4.955 score 0.294 lr 2.11482e-05 
09/12/2020 23:18:30 - INFO - volta.utils -   [GQA]: iter 3925 Ep: 1.07 loss 4.901 score 0.308 lr 2.12568e-05 
09/12/2020 23:20:39 - INFO - volta.utils -   [GQA]: iter 3945 Ep: 1.07 loss 4.952 score 0.293 lr 2.13654e-05 
09/12/2020 23:22:32 - INFO - volta.utils -   [GQA]: iter 3965 Ep: 1.08 loss 4.825 score 0.313 lr 2.14739e-05 
09/12/2020 23:24:37 - INFO - volta.utils -   [GQA]: iter 3985 Ep: 1.08 loss 4.904 score 0.291 lr 2.15825e-05 
09/12/2020 23:26:06 - INFO - volta.utils -   [GQA]: iter 4005 Ep: 1.09 loss 4.776 score 0.306 lr 2.16911e-05 
09/12/2020 23:27:47 - INFO - volta.utils -   [GQA]: iter 4025 Ep: 1.09 loss 4.882 score 0.300 lr 2.17997e-05 
09/12/2020 23:29:43 - INFO - volta.utils -   [GQA]: iter 4045 Ep: 1.10 loss 4.740 score 0.303 lr 2.19083e-05 
09/12/2020 23:31:17 - INFO - volta.utils -   [GQA]: iter 4065 Ep: 1.10 loss 4.752 score 0.309 lr 2.20168e-05 
09/12/2020 23:32:42 - INFO - volta.utils -   [GQA]: iter 4085 Ep: 1.11 loss 4.713 score 0.312 lr 2.21254e-05 
09/12/2020 23:34:19 - INFO - volta.utils -   [GQA]: iter 4105 Ep: 1.11 loss 4.683 score 0.320 lr 2.2234e-05 
09/12/2020 23:36:38 - INFO - volta.utils -   [GQA]: iter 4125 Ep: 1.12 loss 4.715 score 0.319 lr 2.23426e-05 
09/12/2020 23:38:09 - INFO - volta.utils -   [GQA]: iter 4145 Ep: 1.13 loss 4.644 score 0.315 lr 2.24511e-05 
09/12/2020 23:39:33 - INFO - volta.utils -   [GQA]: iter 4165 Ep: 1.13 loss 4.702 score 0.316 lr 2.25597e-05 
09/12/2020 23:41:21 - INFO - volta.utils -   [GQA]: iter 4185 Ep: 1.14 loss 4.623 score 0.305 lr 2.26683e-05 
09/12/2020 23:43:00 - INFO - volta.utils -   [GQA]: iter 4205 Ep: 1.14 loss 4.548 score 0.329 lr 2.27769e-05 
09/12/2020 23:44:41 - INFO - volta.utils -   [GQA]: iter 4225 Ep: 1.15 loss 4.501 score 0.333 lr 2.28855e-05 
09/12/2020 23:46:07 - INFO - volta.utils -   [GQA]: iter 4245 Ep: 1.15 loss 4.563 score 0.330 lr 2.2994e-05 
09/12/2020 23:48:39 - INFO - volta.utils -   [GQA]: iter 4265 Ep: 1.16 loss 4.424 score 0.335 lr 2.31026e-05 
09/12/2020 23:50:06 - INFO - volta.utils -   [GQA]: iter 4285 Ep: 1.16 loss 4.477 score 0.331 lr 2.32112e-05 
09/12/2020 23:51:09 - INFO - volta.utils -   [GQA]: iter 4305 Ep: 1.17 loss 4.452 score 0.334 lr 2.33198e-05 
09/12/2020 23:52:31 - INFO - volta.utils -   [GQA]: iter 4325 Ep: 1.17 loss 4.456 score 0.338 lr 2.34283e-05 
09/12/2020 23:54:27 - INFO - volta.utils -   [GQA]: iter 4345 Ep: 1.18 loss 4.363 score 0.343 lr 2.35369e-05 
09/12/2020 23:55:59 - INFO - volta.utils -   [GQA]: iter 4365 Ep: 1.18 loss 4.368 score 0.339 lr 2.36455e-05 
09/12/2020 23:57:31 - INFO - volta.utils -   [GQA]: iter 4385 Ep: 1.19 loss 4.322 score 0.350 lr 2.37541e-05 
09/12/2020 23:58:47 - INFO - volta.utils -   [GQA]: iter 4405 Ep: 1.20 loss 4.301 score 0.346 lr 2.38626e-05 
09/13/2020 00:01:13 - INFO - volta.utils -   [GQA]: iter 4425 Ep: 1.20 loss 4.245 score 0.352 lr 2.39712e-05 
09/13/2020 00:02:47 - INFO - volta.utils -   [GQA]: iter 4445 Ep: 1.21 loss 4.276 score 0.354 lr 2.40798e-05 
09/13/2020 00:04:19 - INFO - volta.utils -   [GQA]: iter 4465 Ep: 1.21 loss 4.247 score 0.352 lr 2.41884e-05 
09/13/2020 00:05:39 - INFO - volta.utils -   [GQA]: iter 4485 Ep: 1.22 loss 4.178 score 0.362 lr 2.4297e-05 
09/13/2020 00:08:10 - INFO - volta.utils -   [GQA]: iter 4505 Ep: 1.22 loss 4.167 score 0.347 lr 2.44055e-05 
09/13/2020 00:09:38 - INFO - volta.utils -   [GQA]: iter 4525 Ep: 1.23 loss 4.143 score 0.361 lr 2.45141e-05 
09/13/2020 00:11:06 - INFO - volta.utils -   [GQA]: iter 4545 Ep: 1.23 loss 4.118 score 0.356 lr 2.46227e-05 
09/13/2020 00:12:33 - INFO - volta.utils -   [GQA]: iter 4565 Ep: 1.24 loss 4.089 score 0.363 lr 2.47313e-05 
09/13/2020 00:14:27 - INFO - volta.utils -   [GQA]: iter 4585 Ep: 1.24 loss 4.108 score 0.373 lr 2.48398e-05 
09/13/2020 00:15:27 - INFO - volta.utils -   [GQA]: iter 4605 Ep: 1.25 loss 4.025 score 0.375 lr 2.49484e-05 
09/13/2020 00:16:58 - INFO - volta.utils -   [GQA]: iter 4625 Ep: 1.26 loss 4.165 score 0.376 lr 2.5057e-05 
09/13/2020 00:18:06 - INFO - volta.utils -   [GQA]: iter 4645 Ep: 1.26 loss 4.005 score 0.371 lr 2.51656e-05 
09/13/2020 00:20:18 - INFO - volta.utils -   [GQA]: iter 4665 Ep: 1.27 loss 3.977 score 0.386 lr 2.52742e-05 
09/13/2020 00:21:55 - INFO - volta.utils -   [GQA]: iter 4685 Ep: 1.27 loss 3.974 score 0.389 lr 2.53827e-05 
09/13/2020 00:23:43 - INFO - volta.utils -   [GQA]: iter 4705 Ep: 1.28 loss 3.946 score 0.382 lr 2.54913e-05 
09/13/2020 00:25:24 - INFO - volta.utils -   [GQA]: iter 4725 Ep: 1.28 loss 3.915 score 0.375 lr 2.55999e-05 
09/13/2020 00:28:34 - INFO - volta.utils -   [GQA]: iter 4745 Ep: 1.29 loss 3.944 score 0.381 lr 2.57085e-05 
09/13/2020 00:29:39 - INFO - volta.utils -   [GQA]: iter 4765 Ep: 1.29 loss 3.875 score 0.389 lr 2.5817e-05 
09/13/2020 00:31:21 - INFO - volta.utils -   [GQA]: iter 4785 Ep: 1.30 loss 3.867 score 0.385 lr 2.59256e-05 
09/13/2020 00:32:13 - INFO - volta.utils -   [GQA]: iter 4805 Ep: 1.30 loss 3.838 score 0.392 lr 2.60342e-05 
09/13/2020 00:34:31 - INFO - volta.utils -   [GQA]: iter 4825 Ep: 1.31 loss 3.819 score 0.403 lr 2.61428e-05 
09/13/2020 00:36:04 - INFO - volta.utils -   [GQA]: iter 4845 Ep: 1.32 loss 3.828 score 0.388 lr 2.62514e-05 
09/13/2020 00:37:17 - INFO - volta.utils -   [GQA]: iter 4865 Ep: 1.32 loss 3.799 score 0.375 lr 2.63599e-05 
09/13/2020 00:38:20 - INFO - volta.utils -   [GQA]: iter 4885 Ep: 1.33 loss 3.791 score 0.398 lr 2.64685e-05 
09/13/2020 00:40:52 - INFO - volta.utils -   [GQA]: iter 4905 Ep: 1.33 loss 3.722 score 0.399 lr 2.65771e-05 
09/13/2020 00:41:56 - INFO - volta.utils -   [GQA]: iter 4925 Ep: 1.34 loss 3.715 score 0.397 lr 2.66857e-05 
09/13/2020 00:43:54 - INFO - volta.utils -   [GQA]: iter 4945 Ep: 1.34 loss 3.717 score 0.412 lr 2.67942e-05 
09/13/2020 00:45:06 - INFO - volta.utils -   [GQA]: iter 4965 Ep: 1.35 loss 3.699 score 0.405 lr 2.69028e-05 
09/13/2020 00:47:23 - INFO - volta.utils -   [GQA]: iter 4985 Ep: 1.35 loss 3.771 score 0.392 lr 2.70114e-05 
09/13/2020 00:48:46 - INFO - volta.utils -   [GQA]: iter 5005 Ep: 1.36 loss 3.682 score 0.407 lr 2.712e-05 
09/13/2020 00:50:43 - INFO - volta.utils -   [GQA]: iter 5025 Ep: 1.36 loss 3.686 score 0.407 lr 2.72286e-05 
09/13/2020 00:51:44 - INFO - volta.utils -   [GQA]: iter 5045 Ep: 1.37 loss 3.588 score 0.416 lr 2.73371e-05 
09/13/2020 00:52:55 - INFO - volta.utils -   [GQA]: iter 5065 Ep: 1.37 loss 3.577 score 0.411 lr 2.74457e-05 
09/13/2020 00:54:11 - INFO - volta.utils -   [GQA]: iter 5085 Ep: 1.38 loss 3.520 score 0.414 lr 2.75543e-05 
09/13/2020 00:56:22 - INFO - volta.utils -   [GQA]: iter 5105 Ep: 1.39 loss 3.528 score 0.421 lr 2.76629e-05 
09/13/2020 00:57:44 - INFO - volta.utils -   [GQA]: iter 5125 Ep: 1.39 loss 3.516 score 0.426 lr 2.77714e-05 
09/13/2020 00:59:21 - INFO - volta.utils -   [GQA]: iter 5145 Ep: 1.40 loss 3.527 score 0.414 lr 2.788e-05 
09/13/2020 01:00:35 - INFO - volta.utils -   [GQA]: iter 5165 Ep: 1.40 loss 3.463 score 0.435 lr 2.79886e-05 
09/13/2020 01:02:39 - INFO - volta.utils -   [GQA]: iter 5185 Ep: 1.41 loss 3.496 score 0.415 lr 2.80972e-05 
09/13/2020 01:04:05 - INFO - volta.utils -   [GQA]: iter 5205 Ep: 1.41 loss 3.431 score 0.425 lr 2.82058e-05 
09/13/2020 01:05:37 - INFO - volta.utils -   [GQA]: iter 5225 Ep: 1.42 loss 3.498 score 0.423 lr 2.83143e-05 
09/13/2020 01:07:18 - INFO - volta.utils -   [GQA]: iter 5245 Ep: 1.42 loss 3.432 score 0.424 lr 2.84229e-05 
09/13/2020 01:10:22 - INFO - volta.utils -   [GQA]: iter 5265 Ep: 1.43 loss 3.371 score 0.431 lr 2.85315e-05 
09/13/2020 01:11:57 - INFO - volta.utils -   [GQA]: iter 5285 Ep: 1.43 loss 3.417 score 0.426 lr 2.86401e-05 
09/13/2020 01:13:59 - INFO - volta.utils -   [GQA]: iter 5305 Ep: 1.44 loss 3.369 score 0.431 lr 2.87486e-05 
09/13/2020 01:15:48 - INFO - volta.utils -   [GQA]: iter 5325 Ep: 1.45 loss 3.376 score 0.421 lr 2.88572e-05 
09/13/2020 01:19:02 - INFO - volta.utils -   [GQA]: iter 5345 Ep: 1.45 loss 3.412 score 0.426 lr 2.89658e-05 
09/13/2020 01:20:58 - INFO - volta.utils -   [GQA]: iter 5365 Ep: 1.46 loss 3.362 score 0.435 lr 2.90744e-05 
09/13/2020 01:22:33 - INFO - volta.utils -   [GQA]: iter 5385 Ep: 1.46 loss 3.341 score 0.439 lr 2.9183e-05 
09/13/2020 01:23:29 - INFO - volta.utils -   [GQA]: iter 5405 Ep: 1.47 loss 3.389 score 0.430 lr 2.92915e-05 
09/13/2020 01:25:49 - INFO - volta.utils -   [GQA]: iter 5425 Ep: 1.47 loss 3.318 score 0.448 lr 2.94001e-05 
09/13/2020 01:27:22 - INFO - volta.utils -   [GQA]: iter 5445 Ep: 1.48 loss 3.305 score 0.428 lr 2.95087e-05 
09/13/2020 01:28:55 - INFO - volta.utils -   [GQA]: iter 5465 Ep: 1.48 loss 3.286 score 0.446 lr 2.96173e-05 
09/13/2020 01:30:41 - INFO - volta.utils -   [GQA]: iter 5485 Ep: 1.49 loss 3.284 score 0.439 lr 2.97258e-05 
09/13/2020 01:32:15 - INFO - volta.utils -   [GQA]: iter 5505 Ep: 1.49 loss 3.280 score 0.448 lr 2.98344e-05 
09/13/2020 01:34:24 - INFO - volta.utils -   [GQA]: iter 5525 Ep: 1.50 loss 3.246 score 0.435 lr 2.9943e-05 
09/13/2020 01:35:54 - INFO - volta.utils -   [GQA]: iter 5545 Ep: 1.51 loss 3.214 score 0.443 lr 3.00516e-05 
09/13/2020 01:37:38 - INFO - volta.utils -   [GQA]: iter 5565 Ep: 1.51 loss 3.269 score 0.439 lr 3.01602e-05 
09/13/2020 01:39:14 - INFO - volta.utils -   [GQA]: iter 5585 Ep: 1.52 loss 3.222 score 0.448 lr 3.02687e-05 
09/13/2020 01:41:56 - INFO - volta.utils -   [GQA]: iter 5605 Ep: 1.52 loss 3.177 score 0.449 lr 3.03773e-05 
09/13/2020 01:43:02 - INFO - volta.utils -   [GQA]: iter 5625 Ep: 1.53 loss 3.220 score 0.441 lr 3.04859e-05 
09/13/2020 01:44:30 - INFO - volta.utils -   [GQA]: iter 5645 Ep: 1.53 loss 3.163 score 0.454 lr 3.05945e-05 
09/13/2020 01:46:04 - INFO - volta.utils -   [GQA]: iter 5665 Ep: 1.54 loss 3.145 score 0.446 lr 3.0703e-05 
09/13/2020 01:49:02 - INFO - volta.utils -   [GQA]: iter 5685 Ep: 1.54 loss 3.146 score 0.446 lr 3.08116e-05 
09/13/2020 01:50:35 - INFO - volta.utils -   [GQA]: iter 5705 Ep: 1.55 loss 3.103 score 0.453 lr 3.09202e-05 
09/13/2020 01:52:02 - INFO - volta.utils -   [GQA]: iter 5725 Ep: 1.55 loss 3.139 score 0.442 lr 3.10288e-05 
09/13/2020 01:53:56 - INFO - volta.utils -   [GQA]: iter 5745 Ep: 1.56 loss 3.095 score 0.460 lr 3.11374e-05 
09/13/2020 01:56:18 - INFO - volta.utils -   [GQA]: iter 5765 Ep: 1.56 loss 3.102 score 0.447 lr 3.12459e-05 
09/13/2020 01:58:15 - INFO - volta.utils -   [GQA]: iter 5785 Ep: 1.57 loss 3.054 score 0.462 lr 3.13545e-05 
09/13/2020 01:59:55 - INFO - volta.utils -   [GQA]: iter 5805 Ep: 1.58 loss 3.051 score 0.453 lr 3.14631e-05 
09/13/2020 02:01:40 - INFO - volta.utils -   [GQA]: iter 5825 Ep: 1.58 loss 3.033 score 0.467 lr 3.15717e-05 
09/13/2020 02:04:28 - INFO - volta.utils -   [GQA]: iter 5845 Ep: 1.59 loss 3.019 score 0.457 lr 3.16802e-05 
09/13/2020 02:06:25 - INFO - volta.utils -   [GQA]: iter 5865 Ep: 1.59 loss 3.021 score 0.464 lr 3.17888e-05 
09/13/2020 02:08:09 - INFO - volta.utils -   [GQA]: iter 5885 Ep: 1.60 loss 3.042 score 0.454 lr 3.18974e-05 
09/13/2020 02:10:01 - INFO - volta.utils -   [GQA]: iter 5905 Ep: 1.60 loss 2.968 score 0.464 lr 3.2006e-05 
09/13/2020 02:12:52 - INFO - volta.utils -   [GQA]: iter 5925 Ep: 1.61 loss 2.980 score 0.469 lr 3.21145e-05 
09/13/2020 02:14:35 - INFO - volta.utils -   [GQA]: iter 5945 Ep: 1.61 loss 2.935 score 0.472 lr 3.22231e-05 
09/13/2020 02:16:13 - INFO - volta.utils -   [GQA]: iter 5965 Ep: 1.62 loss 3.033 score 0.451 lr 3.23317e-05 
09/13/2020 02:17:20 - INFO - volta.utils -   [GQA]: iter 5985 Ep: 1.62 loss 2.979 score 0.465 lr 3.24403e-05 
09/13/2020 02:20:00 - INFO - volta.utils -   [GQA]: iter 6005 Ep: 1.63 loss 2.982 score 0.459 lr 3.25489e-05 
09/13/2020 02:21:14 - INFO - volta.utils -   [GQA]: iter 6025 Ep: 1.64 loss 2.951 score 0.461 lr 3.26574e-05 
09/13/2020 02:22:37 - INFO - volta.utils -   [GQA]: iter 6045 Ep: 1.64 loss 2.921 score 0.474 lr 3.2766e-05 
09/13/2020 02:24:03 - INFO - volta.utils -   [GQA]: iter 6065 Ep: 1.65 loss 2.926 score 0.466 lr 3.28746e-05 
09/13/2020 02:26:41 - INFO - volta.utils -   [GQA]: iter 6085 Ep: 1.65 loss 2.887 score 0.461 lr 3.29832e-05 
09/13/2020 02:27:59 - INFO - volta.utils -   [GQA]: iter 6105 Ep: 1.66 loss 2.868 score 0.477 lr 3.30917e-05 
09/13/2020 02:29:12 - INFO - volta.utils -   [GQA]: iter 6125 Ep: 1.66 loss 2.901 score 0.484 lr 3.32003e-05 
09/13/2020 02:31:01 - INFO - volta.utils -   [GQA]: iter 6145 Ep: 1.67 loss 2.791 score 0.481 lr 3.33089e-05 
09/13/2020 02:33:31 - INFO - volta.utils -   [GQA]: iter 6165 Ep: 1.67 loss 2.842 score 0.476 lr 3.34175e-05 
09/13/2020 02:34:37 - INFO - volta.utils -   [GQA]: iter 6185 Ep: 1.68 loss 2.821 score 0.479 lr 3.35261e-05 
09/13/2020 02:35:36 - INFO - volta.utils -   [GQA]: iter 6205 Ep: 1.68 loss 2.930 score 0.466 lr 3.36346e-05 
09/13/2020 02:37:07 - INFO - volta.utils -   [GQA]: iter 6225 Ep: 1.69 loss 2.899 score 0.466 lr 3.37432e-05 
09/13/2020 02:39:09 - INFO - volta.utils -   [GQA]: iter 6245 Ep: 1.70 loss 2.878 score 0.469 lr 3.38518e-05 
09/13/2020 02:40:51 - INFO - volta.utils -   [GQA]: iter 6265 Ep: 1.70 loss 2.799 score 0.485 lr 3.39604e-05 
09/13/2020 02:42:35 - INFO - volta.utils -   [GQA]: iter 6285 Ep: 1.71 loss 2.817 score 0.469 lr 3.40689e-05 
09/13/2020 02:44:15 - INFO - volta.utils -   [GQA]: iter 6305 Ep: 1.71 loss 2.792 score 0.477 lr 3.41775e-05 
09/13/2020 02:47:20 - INFO - volta.utils -   [GQA]: iter 6325 Ep: 1.72 loss 2.814 score 0.472 lr 3.42861e-05 
09/13/2020 02:48:39 - INFO - volta.utils -   [GQA]: iter 6345 Ep: 1.72 loss 2.747 score 0.488 lr 3.43947e-05 
09/13/2020 02:50:09 - INFO - volta.utils -   [GQA]: iter 6365 Ep: 1.73 loss 2.767 score 0.483 lr 3.45033e-05 
09/13/2020 02:51:39 - INFO - volta.utils -   [GQA]: iter 6385 Ep: 1.73 loss 2.820 score 0.472 lr 3.46118e-05 
09/13/2020 02:54:32 - INFO - volta.utils -   [GQA]: iter 6405 Ep: 1.74 loss 2.826 score 0.478 lr 3.47204e-05 
09/13/2020 02:56:23 - INFO - volta.utils -   [GQA]: iter 6425 Ep: 1.74 loss 2.778 score 0.482 lr 3.4829e-05 
09/13/2020 02:58:12 - INFO - volta.utils -   [GQA]: iter 6445 Ep: 1.75 loss 2.701 score 0.507 lr 3.49376e-05 
09/13/2020 02:59:31 - INFO - volta.utils -   [GQA]: iter 6465 Ep: 1.75 loss 2.752 score 0.484 lr 3.50461e-05 
09/13/2020 03:02:13 - INFO - volta.utils -   [GQA]: iter 6485 Ep: 1.76 loss 2.833 score 0.475 lr 3.51547e-05 
09/13/2020 03:03:41 - INFO - volta.utils -   [GQA]: iter 6505 Ep: 1.77 loss 2.675 score 0.505 lr 3.52633e-05 
09/13/2020 03:04:59 - INFO - volta.utils -   [GQA]: iter 6525 Ep: 1.77 loss 2.688 score 0.495 lr 3.53719e-05 
09/13/2020 03:06:38 - INFO - volta.utils -   [GQA]: iter 6545 Ep: 1.78 loss 2.746 score 0.484 lr 3.54805e-05 
09/13/2020 03:08:12 - INFO - volta.utils -   [GQA]: iter 6565 Ep: 1.78 loss 2.668 score 0.500 lr 3.5589e-05 
09/13/2020 03:10:27 - INFO - volta.utils -   [GQA]: iter 6585 Ep: 1.79 loss 2.636 score 0.507 lr 3.56976e-05 
09/13/2020 03:11:28 - INFO - volta.utils -   [GQA]: iter 6605 Ep: 1.79 loss 2.698 score 0.493 lr 3.58062e-05 
09/13/2020 03:13:07 - INFO - volta.utils -   [GQA]: iter 6625 Ep: 1.80 loss 2.696 score 0.497 lr 3.59148e-05 
09/13/2020 03:14:30 - INFO - volta.utils -   [GQA]: iter 6645 Ep: 1.80 loss 2.692 score 0.496 lr 3.60233e-05 
09/13/2020 03:16:32 - INFO - volta.utils -   [GQA]: iter 6665 Ep: 1.81 loss 2.596 score 0.507 lr 3.61319e-05 
09/13/2020 03:17:43 - INFO - volta.utils -   [GQA]: iter 6685 Ep: 1.81 loss 2.636 score 0.502 lr 3.62405e-05 
09/13/2020 03:19:43 - INFO - volta.utils -   [GQA]: iter 6705 Ep: 1.82 loss 2.652 score 0.503 lr 3.63491e-05 
09/13/2020 03:21:03 - INFO - volta.utils -   [GQA]: iter 6725 Ep: 1.83 loss 2.659 score 0.504 lr 3.64577e-05 
09/13/2020 03:23:06 - INFO - volta.utils -   [GQA]: iter 6745 Ep: 1.83 loss 2.609 score 0.496 lr 3.65662e-05 
09/13/2020 03:24:36 - INFO - volta.utils -   [GQA]: iter 6765 Ep: 1.84 loss 2.614 score 0.492 lr 3.66748e-05 
09/13/2020 03:26:33 - INFO - volta.utils -   [GQA]: iter 6785 Ep: 1.84 loss 2.638 score 0.499 lr 3.67834e-05 
09/13/2020 03:27:50 - INFO - volta.utils -   [GQA]: iter 6805 Ep: 1.85 loss 2.618 score 0.501 lr 3.6892e-05 
09/13/2020 03:30:31 - INFO - volta.utils -   [GQA]: iter 6825 Ep: 1.85 loss 2.645 score 0.488 lr 3.70005e-05 
09/13/2020 03:32:15 - INFO - volta.utils -   [GQA]: iter 6845 Ep: 1.86 loss 2.598 score 0.512 lr 3.71091e-05 
09/13/2020 03:33:43 - INFO - volta.utils -   [GQA]: iter 6865 Ep: 1.86 loss 2.591 score 0.507 lr 3.72177e-05 
09/13/2020 03:35:29 - INFO - volta.utils -   [GQA]: iter 6885 Ep: 1.87 loss 2.524 score 0.514 lr 3.73263e-05 
09/13/2020 03:37:52 - INFO - volta.utils -   [GQA]: iter 6905 Ep: 1.87 loss 2.561 score 0.510 lr 3.74349e-05 
09/13/2020 03:39:40 - INFO - volta.utils -   [GQA]: iter 6925 Ep: 1.88 loss 2.608 score 0.508 lr 3.75434e-05 
09/13/2020 03:41:21 - INFO - volta.utils -   [GQA]: iter 6945 Ep: 1.89 loss 2.651 score 0.487 lr 3.7652e-05 
09/13/2020 03:42:48 - INFO - volta.utils -   [GQA]: iter 6965 Ep: 1.89 loss 2.591 score 0.502 lr 3.77606e-05 
09/13/2020 03:44:59 - INFO - volta.utils -   [GQA]: iter 6985 Ep: 1.90 loss 2.564 score 0.505 lr 3.78692e-05 
09/13/2020 03:46:57 - INFO - volta.utils -   [GQA]: iter 7005 Ep: 1.90 loss 2.577 score 0.509 lr 3.79777e-05 
09/13/2020 03:48:46 - INFO - volta.utils -   [GQA]: iter 7025 Ep: 1.91 loss 2.563 score 0.506 lr 3.80863e-05 
09/13/2020 03:49:38 - INFO - volta.utils -   [GQA]: iter 7045 Ep: 1.91 loss 2.534 score 0.512 lr 3.81949e-05 
09/13/2020 03:52:54 - INFO - volta.utils -   [GQA]: iter 7065 Ep: 1.92 loss 2.586 score 0.507 lr 3.83035e-05 
09/13/2020 03:53:28 - INFO - volta.utils -   [GQA]: iter 7085 Ep: 1.92 loss 2.508 score 0.522 lr 3.84121e-05 
09/13/2020 03:54:45 - INFO - volta.utils -   [GQA]: iter 7105 Ep: 1.93 loss 2.564 score 0.512 lr 3.85206e-05 
09/13/2020 03:56:23 - INFO - volta.utils -   [GQA]: iter 7125 Ep: 1.93 loss 2.550 score 0.508 lr 3.86292e-05 
09/13/2020 03:58:55 - INFO - volta.utils -   [GQA]: iter 7145 Ep: 1.94 loss 2.507 score 0.511 lr 3.87378e-05 
09/13/2020 04:00:08 - INFO - volta.utils -   [GQA]: iter 7165 Ep: 1.94 loss 2.526 score 0.511 lr 3.88464e-05 
09/13/2020 04:01:34 - INFO - volta.utils -   [GQA]: iter 7185 Ep: 1.95 loss 2.531 score 0.523 lr 3.89549e-05 
09/13/2020 04:02:23 - INFO - volta.utils -   [GQA]: iter 7205 Ep: 1.96 loss 2.519 score 0.513 lr 3.90635e-05 
09/13/2020 04:04:57 - INFO - volta.utils -   [GQA]: iter 7225 Ep: 1.96 loss 2.432 score 0.520 lr 3.91721e-05 
09/13/2020 04:06:07 - INFO - volta.utils -   [GQA]: iter 7245 Ep: 1.97 loss 2.514 score 0.511 lr 3.92807e-05 
09/13/2020 04:07:22 - INFO - volta.utils -   [GQA]: iter 7265 Ep: 1.97 loss 2.553 score 0.520 lr 3.93893e-05 
09/13/2020 04:08:27 - INFO - volta.utils -   [GQA]: iter 7285 Ep: 1.98 loss 2.453 score 0.521 lr 3.94978e-05 
09/13/2020 04:10:39 - INFO - volta.utils -   [GQA]: iter 7305 Ep: 1.98 loss 2.516 score 0.509 lr 3.96064e-05 
09/13/2020 04:11:51 - INFO - volta.utils -   [GQA]: iter 7325 Ep: 1.99 loss 2.395 score 0.535 lr 3.9715e-05 
09/13/2020 04:13:13 - INFO - volta.utils -   [GQA]: iter 7345 Ep: 1.99 loss 2.473 score 0.521 lr 3.98236e-05 
09/13/2020 04:14:11 - INFO - volta.utils -   [GQA]: iter 7365 Ep: 2.00 loss 2.529 score 0.510 lr 3.99321e-05 
09/13/2020 04:14:44 - INFO - __main__ -   ** ** * Saving fine - tuned model ** ** * 

Epoch:  10%|█         | 2/20 [11:17:40<102:22:48, 20476.03s/it]09/13/2020 04:35:12 - INFO - volta.utils -   Eval task TASK15 on iteration 7369 
09/13/2020 04:35:12 - INFO - volta.utils -   Validation [GQA]: loss 2.402 score 52.369 
09/13/2020 04:35:25 - INFO - volta.utils -   [GQA]: iter 7389 Ep: 2.01 loss 2.371 score 0.527 lr 3.99935e-05 
09/13/2020 04:37:01 - INFO - volta.utils -   [GQA]: iter 7409 Ep: 2.01 loss 2.439 score 0.523 lr 3.9981e-05 
09/13/2020 04:38:17 - INFO - volta.utils -   [GQA]: iter 7429 Ep: 2.02 loss 2.401 score 0.527 lr 3.99689e-05 
09/13/2020 04:39:17 - INFO - volta.utils -   [GQA]: iter 7449 Ep: 2.02 loss 2.418 score 0.533 lr 3.99569e-05 
09/13/2020 04:41:29 - INFO - volta.utils -   [GQA]: iter 7469 Ep: 2.03 loss 2.430 score 0.533 lr 3.99448e-05 
09/13/2020 04:42:43 - INFO - volta.utils -   [GQA]: iter 7489 Ep: 2.03 loss 2.401 score 0.533 lr 3.99327e-05 
09/13/2020 04:44:16 - INFO - volta.utils -   [GQA]: iter 7509 Ep: 2.04 loss 2.388 score 0.527 lr 3.99207e-05 
09/13/2020 04:45:51 - INFO - volta.utils -   [GQA]: iter 7529 Ep: 2.04 loss 2.381 score 0.533 lr 3.99086e-05 
09/13/2020 04:48:00 - INFO - volta.utils -   [GQA]: iter 7549 Ep: 2.05 loss 2.360 score 0.530 lr 3.98965e-05 
09/13/2020 04:50:17 - INFO - volta.utils -   [GQA]: iter 7569 Ep: 2.05 loss 2.410 score 0.530 lr 3.98845e-05 
09/13/2020 04:51:21 - INFO - volta.utils -   [GQA]: iter 7589 Ep: 2.06 loss 2.369 score 0.533 lr 3.98724e-05 
09/13/2020 04:52:45 - INFO - volta.utils -   [GQA]: iter 7609 Ep: 2.07 loss 2.363 score 0.537 lr 3.98604e-05 
09/13/2020 04:55:05 - INFO - volta.utils -   [GQA]: iter 7629 Ep: 2.07 loss 2.369 score 0.529 lr 3.98483e-05 
09/13/2020 04:56:48 - INFO - volta.utils -   [GQA]: iter 7649 Ep: 2.08 loss 2.373 score 0.530 lr 3.98362e-05 
09/13/2020 04:57:55 - INFO - volta.utils -   [GQA]: iter 7669 Ep: 2.08 loss 2.359 score 0.521 lr 3.98242e-05 
09/13/2020 04:59:07 - INFO - volta.utils -   [GQA]: iter 7689 Ep: 2.09 loss 2.311 score 0.541 lr 3.98121e-05 
09/13/2020 05:00:45 - INFO - volta.utils -   [GQA]: iter 7709 Ep: 2.09 loss 2.372 score 0.523 lr 3.98e-05 
09/13/2020 05:03:18 - INFO - volta.utils -   [GQA]: iter 7729 Ep: 2.10 loss 2.318 score 0.542 lr 3.9788e-05 
09/13/2020 05:04:15 - INFO - volta.utils -   [GQA]: iter 7749 Ep: 2.10 loss 2.347 score 0.540 lr 3.97759e-05 
09/13/2020 05:05:37 - INFO - volta.utils -   [GQA]: iter 7769 Ep: 2.11 loss 2.285 score 0.542 lr 3.97638e-05 
09/13/2020 05:07:21 - INFO - volta.utils -   [GQA]: iter 7789 Ep: 2.11 loss 2.342 score 0.528 lr 3.97518e-05 
09/13/2020 05:09:52 - INFO - volta.utils -   [GQA]: iter 7809 Ep: 2.12 loss 2.278 score 0.545 lr 3.97397e-05 
09/13/2020 05:11:22 - INFO - volta.utils -   [GQA]: iter 7829 Ep: 2.13 loss 2.315 score 0.551 lr 3.97277e-05 
09/13/2020 05:12:49 - INFO - volta.utils -   [GQA]: iter 7849 Ep: 2.13 loss 2.279 score 0.549 lr 3.97156e-05 
09/13/2020 05:14:22 - INFO - volta.utils -   [GQA]: iter 7869 Ep: 2.14 loss 2.317 score 0.542 lr 3.97035e-05 
09/13/2020 05:16:27 - INFO - volta.utils -   [GQA]: iter 7889 Ep: 2.14 loss 2.312 score 0.536 lr 3.96915e-05 
09/13/2020 05:18:24 - INFO - volta.utils -   [GQA]: iter 7909 Ep: 2.15 loss 2.294 score 0.548 lr 3.96794e-05 
09/13/2020 05:20:08 - INFO - volta.utils -   [GQA]: iter 7929 Ep: 2.15 loss 2.324 score 0.536 lr 3.96673e-05 
09/13/2020 05:21:42 - INFO - volta.utils -   [GQA]: iter 7949 Ep: 2.16 loss 2.271 score 0.538 lr 3.96553e-05 
09/13/2020 05:23:43 - INFO - volta.utils -   [GQA]: iter 7969 Ep: 2.16 loss 2.311 score 0.549 lr 3.96432e-05 
09/13/2020 05:26:08 - INFO - volta.utils -   [GQA]: iter 7989 Ep: 2.17 loss 2.267 score 0.549 lr 3.96311e-05 
09/13/2020 05:28:02 - INFO - volta.utils -   [GQA]: iter 8009 Ep: 2.17 loss 2.270 score 0.552 lr 3.96191e-05 
09/13/2020 05:29:46 - INFO - volta.utils -   [GQA]: iter 8029 Ep: 2.18 loss 2.306 score 0.543 lr 3.9607e-05 
09/13/2020 05:31:52 - INFO - volta.utils -   [GQA]: iter 8049 Ep: 2.18 loss 2.305 score 0.545 lr 3.95949e-05 
09/13/2020 05:33:32 - INFO - volta.utils -   [GQA]: iter 8069 Ep: 2.19 loss 2.322 score 0.543 lr 3.95829e-05 
09/13/2020 05:35:10 - INFO - volta.utils -   [GQA]: iter 8089 Ep: 2.20 loss 2.253 score 0.552 lr 3.95708e-05 
09/13/2020 05:36:52 - INFO - volta.utils -   [GQA]: iter 8109 Ep: 2.20 loss 2.275 score 0.543 lr 3.95588e-05 
09/13/2020 05:39:18 - INFO - volta.utils -   [GQA]: iter 8129 Ep: 2.21 loss 2.306 score 0.540 lr 3.95467e-05 
09/13/2020 05:40:01 - INFO - volta.utils -   [GQA]: iter 8149 Ep: 2.21 loss 2.248 score 0.550 lr 3.95346e-05 
09/13/2020 05:41:45 - INFO - volta.utils -   [GQA]: iter 8169 Ep: 2.22 loss 2.204 score 0.563 lr 3.95226e-05 
09/13/2020 05:43:18 - INFO - volta.utils -   [GQA]: iter 8189 Ep: 2.22 loss 2.266 score 0.540 lr 3.95105e-05 
09/13/2020 05:45:31 - INFO - volta.utils -   [GQA]: iter 8209 Ep: 2.23 loss 2.232 score 0.559 lr 3.94984e-05 
09/13/2020 05:47:01 - INFO - volta.utils -   [GQA]: iter 8229 Ep: 2.23 loss 2.256 score 0.539 lr 3.94864e-05 
09/13/2020 05:48:58 - INFO - volta.utils -   [GQA]: iter 8249 Ep: 2.24 loss 2.203 score 0.554 lr 3.94743e-05 
09/13/2020 05:50:21 - INFO - volta.utils -   [GQA]: iter 8269 Ep: 2.24 loss 2.236 score 0.553 lr 3.94622e-05 
09/13/2020 05:52:16 - INFO - volta.utils -   [GQA]: iter 8289 Ep: 2.25 loss 2.238 score 0.555 lr 3.94502e-05 
09/13/2020 05:53:35 - INFO - volta.utils -   [GQA]: iter 8309 Ep: 2.26 loss 2.232 score 0.547 lr 3.94381e-05 
09/13/2020 05:54:55 - INFO - volta.utils -   [GQA]: iter 8329 Ep: 2.26 loss 2.226 score 0.551 lr 3.9426e-05 
09/13/2020 05:57:08 - INFO - volta.utils -   [GQA]: iter 8349 Ep: 2.27 loss 2.217 score 0.553 lr 3.9414e-05 
09/13/2020 05:58:35 - INFO - volta.utils -   [GQA]: iter 8369 Ep: 2.27 loss 2.263 score 0.553 lr 3.94019e-05 
09/13/2020 05:59:39 - INFO - volta.utils -   [GQA]: iter 8389 Ep: 2.28 loss 2.239 score 0.549 lr 3.93899e-05 
09/13/2020 06:01:18 - INFO - volta.utils -   [GQA]: iter 8409 Ep: 2.28 loss 2.211 score 0.557 lr 3.93778e-05 
09/13/2020 06:04:08 - INFO - volta.utils -   [GQA]: iter 8429 Ep: 2.29 loss 2.215 score 0.551 lr 3.93657e-05 
09/13/2020 06:05:23 - INFO - volta.utils -   [GQA]: iter 8449 Ep: 2.29 loss 2.230 score 0.546 lr 3.93537e-05 
09/13/2020 06:06:52 - INFO - volta.utils -   [GQA]: iter 8469 Ep: 2.30 loss 2.291 score 0.547 lr 3.93416e-05 
09/13/2020 06:08:49 - INFO - volta.utils -   [GQA]: iter 8489 Ep: 2.30 loss 2.223 score 0.549 lr 3.93295e-05 
09/13/2020 06:10:52 - INFO - volta.utils -   [GQA]: iter 8509 Ep: 2.31 loss 2.227 score 0.540 lr 3.93175e-05 
09/13/2020 06:12:35 - INFO - volta.utils -   [GQA]: iter 8529 Ep: 2.32 loss 2.212 score 0.550 lr 3.93054e-05 
09/13/2020 06:14:24 - INFO - volta.utils -   [GQA]: iter 8549 Ep: 2.32 loss 2.164 score 0.559 lr 3.92933e-05 
09/13/2020 06:16:08 - INFO - volta.utils -   [GQA]: iter 8569 Ep: 2.33 loss 2.211 score 0.556 lr 3.92813e-05 
09/13/2020 06:19:21 - INFO - volta.utils -   [GQA]: iter 8589 Ep: 2.33 loss 2.216 score 0.545 lr 3.92692e-05 
09/13/2020 06:20:23 - INFO - volta.utils -   [GQA]: iter 8609 Ep: 2.34 loss 2.182 score 0.562 lr 3.92571e-05 
09/13/2020 06:22:27 - INFO - volta.utils -   [GQA]: iter 8629 Ep: 2.34 loss 2.179 score 0.559 lr 3.92451e-05 
09/13/2020 06:23:53 - INFO - volta.utils -   [GQA]: iter 8649 Ep: 2.35 loss 2.175 score 0.562 lr 3.9233e-05 
09/13/2020 06:26:45 - INFO - volta.utils -   [GQA]: iter 8669 Ep: 2.35 loss 2.170 score 0.563 lr 3.9221e-05 
09/13/2020 06:28:10 - INFO - volta.utils -   [GQA]: iter 8689 Ep: 2.36 loss 2.161 score 0.559 lr 3.92089e-05 
09/13/2020 06:29:42 - INFO - volta.utils -   [GQA]: iter 8709 Ep: 2.36 loss 2.158 score 0.569 lr 3.91968e-05 
09/13/2020 06:31:30 - INFO - volta.utils -   [GQA]: iter 8729 Ep: 2.37 loss 2.165 score 0.555 lr 3.91848e-05 
09/13/2020 06:34:38 - INFO - volta.utils -   [GQA]: iter 8749 Ep: 2.37 loss 2.198 score 0.561 lr 3.91727e-05 
09/13/2020 06:36:18 - INFO - volta.utils -   [GQA]: iter 8769 Ep: 2.38 loss 2.145 score 0.565 lr 3.91606e-05 
09/13/2020 06:37:38 - INFO - volta.utils -   [GQA]: iter 8789 Ep: 2.39 loss 2.194 score 0.551 lr 3.91486e-05 
09/13/2020 06:39:18 - INFO - volta.utils -   [GQA]: iter 8809 Ep: 2.39 loss 2.185 score 0.557 lr 3.91365e-05 
09/13/2020 06:41:24 - INFO - volta.utils -   [GQA]: iter 8829 Ep: 2.40 loss 2.102 score 0.572 lr 3.91244e-05 
09/13/2020 06:43:00 - INFO - volta.utils -   [GQA]: iter 8849 Ep: 2.40 loss 2.155 score 0.564 lr 3.91124e-05 
09/13/2020 06:44:34 - INFO - volta.utils -   [GQA]: iter 8869 Ep: 2.41 loss 2.184 score 0.556 lr 3.91003e-05 
09/13/2020 06:46:42 - INFO - volta.utils -   [GQA]: iter 8889 Ep: 2.41 loss 2.229 score 0.542 lr 3.90882e-05 
09/13/2020 06:48:27 - INFO - volta.utils -   [GQA]: iter 8909 Ep: 2.42 loss 2.169 score 0.555 lr 3.90762e-05 
09/13/2020 06:49:58 - INFO - volta.utils -   [GQA]: iter 8929 Ep: 2.42 loss 2.156 score 0.551 lr 3.90641e-05 
09/13/2020 06:51:25 - INFO - volta.utils -   [GQA]: iter 8949 Ep: 2.43 loss 2.147 score 0.564 lr 3.90521e-05 
09/13/2020 06:53:53 - INFO - volta.utils -   [GQA]: iter 8969 Ep: 2.43 loss 2.145 score 0.561 lr 3.904e-05 
09/13/2020 06:55:40 - INFO - volta.utils -   [GQA]: iter 8989 Ep: 2.44 loss 2.157 score 0.564 lr 3.90279e-05 
09/13/2020 06:57:05 - INFO - volta.utils -   [GQA]: iter 9009 Ep: 2.45 loss 2.141 score 0.551 lr 3.90159e-05 
09/13/2020 06:58:12 - INFO - volta.utils -   [GQA]: iter 9029 Ep: 2.45 loss 2.099 score 0.569 lr 3.90038e-05 
09/13/2020 07:00:01 - INFO - volta.utils -   [GQA]: iter 9049 Ep: 2.46 loss 2.109 score 0.557 lr 3.89917e-05 
09/13/2020 07:01:36 - INFO - volta.utils -   [GQA]: iter 9069 Ep: 2.46 loss 2.100 score 0.573 lr 3.89797e-05 
09/13/2020 07:02:58 - INFO - volta.utils -   [GQA]: iter 9089 Ep: 2.47 loss 2.138 score 0.557 lr 3.89676e-05 
09/13/2020 07:04:26 - INFO - volta.utils -   [GQA]: iter 9109 Ep: 2.47 loss 2.100 score 0.567 lr 3.89555e-05 
09/13/2020 07:06:57 - INFO - volta.utils -   [GQA]: iter 9129 Ep: 2.48 loss 2.126 score 0.554 lr 3.89435e-05 
09/13/2020 07:08:17 - INFO - volta.utils -   [GQA]: iter 9149 Ep: 2.48 loss 2.080 score 0.569 lr 3.89314e-05 
09/13/2020 07:09:35 - INFO - volta.utils -   [GQA]: iter 9169 Ep: 2.49 loss 2.136 score 0.565 lr 3.89194e-05 
09/13/2020 07:10:55 - INFO - volta.utils -   [GQA]: iter 9189 Ep: 2.49 loss 2.112 score 0.560 lr 3.89073e-05 
09/13/2020 07:14:10 - INFO - volta.utils -   [GQA]: iter 9209 Ep: 2.50 loss 2.133 score 0.559 lr 3.88952e-05 
09/13/2020 07:15:57 - INFO - volta.utils -   [GQA]: iter 9229 Ep: 2.51 loss 2.081 score 0.562 lr 3.88832e-05 
09/13/2020 07:17:12 - INFO - volta.utils -   [GQA]: iter 9249 Ep: 2.51 loss 2.047 score 0.564 lr 3.88711e-05 
09/13/2020 07:19:02 - INFO - volta.utils -   [GQA]: iter 9269 Ep: 2.52 loss 2.103 score 0.554 lr 3.8859e-05 
09/13/2020 07:21:27 - INFO - volta.utils -   [GQA]: iter 9289 Ep: 2.52 loss 2.092 score 0.564 lr 3.8847e-05 
09/13/2020 07:23:04 - INFO - volta.utils -   [GQA]: iter 9309 Ep: 2.53 loss 2.064 score 0.567 lr 3.88349e-05 
09/13/2020 07:24:58 - INFO - volta.utils -   [GQA]: iter 9329 Ep: 2.53 loss 2.099 score 0.572 lr 3.88228e-05 
09/13/2020 07:27:19 - INFO - volta.utils -   [GQA]: iter 9349 Ep: 2.54 loss 2.090 score 0.563 lr 3.88108e-05 
09/13/2020 07:29:22 - INFO - volta.utils -   [GQA]: iter 9369 Ep: 2.54 loss 2.096 score 0.566 lr 3.87987e-05 
09/13/2020 07:30:57 - INFO - volta.utils -   [GQA]: iter 9389 Ep: 2.55 loss 2.130 score 0.562 lr 3.87866e-05 
09/13/2020 07:32:32 - INFO - volta.utils -   [GQA]: iter 9409 Ep: 2.55 loss 2.019 score 0.579 lr 3.87746e-05 
09/13/2020 07:35:19 - INFO - volta.utils -   [GQA]: iter 9429 Ep: 2.56 loss 2.156 score 0.555 lr 3.87625e-05 
09/13/2020 07:36:59 - INFO - volta.utils -   [GQA]: iter 9449 Ep: 2.56 loss 2.081 score 0.572 lr 3.87505e-05 
09/13/2020 07:38:10 - INFO - volta.utils -   [GQA]: iter 9469 Ep: 2.57 loss 2.079 score 0.566 lr 3.87384e-05 
09/13/2020 07:40:30 - INFO - volta.utils -   [GQA]: iter 9489 Ep: 2.58 loss 2.068 score 0.579 lr 3.87263e-05 
09/13/2020 07:42:20 - INFO - volta.utils -   [GQA]: iter 9509 Ep: 2.58 loss 2.050 score 0.572 lr 3.87143e-05 
09/13/2020 07:44:03 - INFO - volta.utils -   [GQA]: iter 9529 Ep: 2.59 loss 2.069 score 0.576 lr 3.87022e-05 
09/13/2020 07:45:53 - INFO - volta.utils -   [GQA]: iter 9549 Ep: 2.59 loss 2.054 score 0.570 lr 3.86901e-05 
09/13/2020 07:47:29 - INFO - volta.utils -   [GQA]: iter 9569 Ep: 2.60 loss 1.985 score 0.589 lr 3.86781e-05 
09/13/2020 07:49:22 - INFO - volta.utils -   [GQA]: iter 9589 Ep: 2.60 loss 2.062 score 0.575 lr 3.8666e-05 
09/13/2020 07:51:28 - INFO - volta.utils -   [GQA]: iter 9609 Ep: 2.61 loss 2.081 score 0.566 lr 3.86539e-05 
09/13/2020 07:52:50 - INFO - volta.utils -   [GQA]: iter 9629 Ep: 2.61 loss 2.062 score 0.575 lr 3.86419e-05 
09/13/2020 07:54:30 - INFO - volta.utils -   [GQA]: iter 9649 Ep: 2.62 loss 2.057 score 0.579 lr 3.86298e-05 
09/13/2020 07:55:56 - INFO - volta.utils -   [GQA]: iter 9669 Ep: 2.62 loss 2.045 score 0.576 lr 3.86177e-05 
09/13/2020 07:58:24 - INFO - volta.utils -   [GQA]: iter 9689 Ep: 2.63 loss 2.059 score 0.581 lr 3.86057e-05 
09/13/2020 07:59:40 - INFO - volta.utils -   [GQA]: iter 9709 Ep: 2.64 loss 2.055 score 0.573 lr 3.85936e-05 
09/13/2020 08:01:21 - INFO - volta.utils -   [GQA]: iter 9729 Ep: 2.64 loss 2.006 score 0.576 lr 3.85816e-05 
09/13/2020 08:02:46 - INFO - volta.utils -   [GQA]: iter 9749 Ep: 2.65 loss 2.009 score 0.574 lr 3.85695e-05 
09/13/2020 08:05:02 - INFO - volta.utils -   [GQA]: iter 9769 Ep: 2.65 loss 2.064 score 0.571 lr 3.85574e-05 
09/13/2020 08:06:16 - INFO - volta.utils -   [GQA]: iter 9789 Ep: 2.66 loss 2.004 score 0.576 lr 3.85454e-05 
09/13/2020 08:08:02 - INFO - volta.utils -   [GQA]: iter 9809 Ep: 2.66 loss 2.104 score 0.575 lr 3.85333e-05 
09/13/2020 08:09:28 - INFO - volta.utils -   [GQA]: iter 9829 Ep: 2.67 loss 2.078 score 0.566 lr 3.85212e-05 
09/13/2020 08:11:53 - INFO - volta.utils -   [GQA]: iter 9849 Ep: 2.67 loss 2.071 score 0.562 lr 3.85092e-05 
09/13/2020 08:13:27 - INFO - volta.utils -   [GQA]: iter 9869 Ep: 2.68 loss 2.020 score 0.583 lr 3.84971e-05 
09/13/2020 08:15:35 - INFO - volta.utils -   [GQA]: iter 9889 Ep: 2.68 loss 1.985 score 0.595 lr 3.8485e-05 
09/13/2020 08:17:47 - INFO - volta.utils -   [GQA]: iter 9909 Ep: 2.69 loss 2.033 score 0.566 lr 3.8473e-05 
09/13/2020 08:21:01 - INFO - volta.utils -   [GQA]: iter 9929 Ep: 2.70 loss 2.026 score 0.582 lr 3.84609e-05 
09/13/2020 08:22:34 - INFO - volta.utils -   [GQA]: iter 9949 Ep: 2.70 loss 2.013 score 0.578 lr 3.84488e-05 
09/13/2020 08:23:56 - INFO - volta.utils -   [GQA]: iter 9969 Ep: 2.71 loss 1.992 score 0.574 lr 3.84368e-05 
09/13/2020 08:25:12 - INFO - volta.utils -   [GQA]: iter 9989 Ep: 2.71 loss 1.978 score 0.592 lr 3.84247e-05 
09/13/2020 08:28:24 - INFO - volta.utils -   [GQA]: iter 10009 Ep: 2.72 loss 2.000 score 0.584 lr 3.84127e-05 
09/13/2020 08:30:01 - INFO - volta.utils -   [GQA]: iter 10029 Ep: 2.72 loss 1.983 score 0.600 lr 3.84006e-05 
09/13/2020 08:31:41 - INFO - volta.utils -   [GQA]: iter 10049 Ep: 2.73 loss 1.958 score 0.588 lr 3.83885e-05 
09/13/2020 08:33:18 - INFO - volta.utils -   [GQA]: iter 10069 Ep: 2.73 loss 2.038 score 0.569 lr 3.83765e-05 
09/13/2020 08:36:47 - INFO - volta.utils -   [GQA]: iter 10089 Ep: 2.74 loss 2.039 score 0.580 lr 3.83644e-05 
09/13/2020 08:38:54 - INFO - volta.utils -   [GQA]: iter 10109 Ep: 2.74 loss 1.974 score 0.583 lr 3.83523e-05 
09/13/2020 08:40:22 - INFO - volta.utils -   [GQA]: iter 10129 Ep: 2.75 loss 1.975 score 0.585 lr 3.83403e-05 
09/13/2020 08:41:53 - INFO - volta.utils -   [GQA]: iter 10149 Ep: 2.75 loss 1.973 score 0.587 lr 3.83282e-05 
09/13/2020 08:44:21 - INFO - volta.utils -   [GQA]: iter 10169 Ep: 2.76 loss 2.046 score 0.572 lr 3.83161e-05 
09/13/2020 08:45:25 - INFO - volta.utils -   [GQA]: iter 10189 Ep: 2.77 loss 2.017 score 0.568 lr 3.83041e-05 
09/13/2020 08:46:44 - INFO - volta.utils -   [GQA]: iter 10209 Ep: 2.77 loss 1.998 score 0.581 lr 3.8292e-05 
09/13/2020 08:48:31 - INFO - volta.utils -   [GQA]: iter 10229 Ep: 2.78 loss 1.993 score 0.581 lr 3.82799e-05 
09/13/2020 08:51:10 - INFO - volta.utils -   [GQA]: iter 10249 Ep: 2.78 loss 2.011 score 0.579 lr 3.82679e-05 
09/13/2020 08:52:55 - INFO - volta.utils -   [GQA]: iter 10269 Ep: 2.79 loss 1.995 score 0.582 lr 3.82558e-05 
09/13/2020 08:54:48 - INFO - volta.utils -   [GQA]: iter 10289 Ep: 2.79 loss 1.964 score 0.586 lr 3.82438e-05 
09/13/2020 08:55:55 - INFO - volta.utils -   [GQA]: iter 10309 Ep: 2.80 loss 1.981 score 0.580 lr 3.82317e-05 
09/13/2020 08:57:36 - INFO - volta.utils -   [GQA]: iter 10329 Ep: 2.80 loss 2.011 score 0.582 lr 3.82196e-05 
09/13/2020 08:59:21 - INFO - volta.utils -   [GQA]: iter 10349 Ep: 2.81 loss 1.990 score 0.587 lr 3.82076e-05 
09/13/2020 09:01:09 - INFO - volta.utils -   [GQA]: iter 10369 Ep: 2.81 loss 1.969 score 0.591 lr 3.81955e-05 
09/13/2020 09:02:19 - INFO - volta.utils -   [GQA]: iter 10389 Ep: 2.82 loss 1.933 score 0.590 lr 3.81834e-05 
09/13/2020 09:03:43 - INFO - volta.utils -   [GQA]: iter 10409 Ep: 2.83 loss 1.969 score 0.588 lr 3.81714e-05 
09/13/2020 09:05:17 - INFO - volta.utils -   [GQA]: iter 10429 Ep: 2.83 loss 1.959 score 0.589 lr 3.81593e-05 
09/13/2020 09:07:28 - INFO - volta.utils -   [GQA]: iter 10449 Ep: 2.84 loss 1.920 score 0.583 lr 3.81472e-05 
09/13/2020 09:08:49 - INFO - volta.utils -   [GQA]: iter 10469 Ep: 2.84 loss 1.974 score 0.583 lr 3.81352e-05 
09/13/2020 09:10:11 - INFO - volta.utils -   [GQA]: iter 10489 Ep: 2.85 loss 2.011 score 0.579 lr 3.81231e-05 
09/13/2020 09:11:47 - INFO - volta.utils -   [GQA]: iter 10509 Ep: 2.85 loss 1.938 score 0.592 lr 3.81111e-05 
09/13/2020 09:14:50 - INFO - volta.utils -   [GQA]: iter 10529 Ep: 2.86 loss 1.890 score 0.598 lr 3.8099e-05 
09/13/2020 09:15:47 - INFO - volta.utils -   [GQA]: iter 10549 Ep: 2.86 loss 1.965 score 0.593 lr 3.80869e-05 
09/13/2020 09:17:00 - INFO - volta.utils -   [GQA]: iter 10569 Ep: 2.87 loss 1.938 score 0.594 lr 3.80749e-05 
09/13/2020 09:18:34 - INFO - volta.utils -   [GQA]: iter 10589 Ep: 2.87 loss 2.003 score 0.583 lr 3.80628e-05 
09/13/2020 09:21:11 - INFO - volta.utils -   [GQA]: iter 10609 Ep: 2.88 loss 2.025 score 0.579 lr 3.80507e-05 
09/13/2020 09:23:10 - INFO - volta.utils -   [GQA]: iter 10629 Ep: 2.89 loss 1.945 score 0.591 lr 3.80387e-05 
09/13/2020 09:24:51 - INFO - volta.utils -   [GQA]: iter 10649 Ep: 2.89 loss 1.950 score 0.592 lr 3.80266e-05 
09/13/2020 09:26:38 - INFO - volta.utils -   [GQA]: iter 10669 Ep: 2.90 loss 1.946 score 0.593 lr 3.80145e-05 
09/13/2020 09:28:49 - INFO - volta.utils -   [GQA]: iter 10689 Ep: 2.90 loss 1.960 score 0.594 lr 3.80025e-05 
09/13/2020 09:30:34 - INFO - volta.utils -   [GQA]: iter 10709 Ep: 2.91 loss 1.979 score 0.584 lr 3.79904e-05 
09/13/2020 09:32:30 - INFO - volta.utils -   [GQA]: iter 10729 Ep: 2.91 loss 1.934 score 0.598 lr 3.79783e-05 
09/13/2020 09:34:59 - INFO - volta.utils -   [GQA]: iter 10749 Ep: 2.92 loss 1.937 score 0.599 lr 3.79663e-05 
09/13/2020 09:36:35 - INFO - volta.utils -   [GQA]: iter 10769 Ep: 2.92 loss 1.903 score 0.599 lr 3.79542e-05 
09/13/2020 09:38:24 - INFO - volta.utils -   [GQA]: iter 10789 Ep: 2.93 loss 1.949 score 0.596 lr 3.79422e-05 
09/13/2020 09:40:10 - INFO - volta.utils -   [GQA]: iter 10809 Ep: 2.93 loss 1.946 score 0.589 lr 3.79301e-05 
09/13/2020 09:41:40 - INFO - volta.utils -   [GQA]: iter 10829 Ep: 2.94 loss 1.911 score 0.605 lr 3.7918e-05 
09/13/2020 09:43:52 - INFO - volta.utils -   [GQA]: iter 10849 Ep: 2.94 loss 1.892 score 0.606 lr 3.7906e-05 
09/13/2020 09:45:36 - INFO - volta.utils -   [GQA]: iter 10869 Ep: 2.95 loss 1.940 score 0.600 lr 3.78939e-05 
09/13/2020 09:47:21 - INFO - volta.utils -   [GQA]: iter 10889 Ep: 2.96 loss 1.958 score 0.594 lr 3.78818e-05 
09/13/2020 09:49:04 - INFO - volta.utils -   [GQA]: iter 10909 Ep: 2.96 loss 1.982 score 0.596 lr 3.78698e-05 
09/13/2020 09:52:13 - INFO - volta.utils -   [GQA]: iter 10929 Ep: 2.97 loss 1.965 score 0.595 lr 3.78577e-05 
09/13/2020 09:54:58 - INFO - volta.utils -   [GQA]: iter 10949 Ep: 2.97 loss 1.950 score 0.601 lr 3.78456e-05 
09/13/2020 09:56:15 - INFO - volta.utils -   [GQA]: iter 10969 Ep: 2.98 loss 1.905 score 0.608 lr 3.78336e-05 
09/13/2020 09:57:19 - INFO - volta.utils -   [GQA]: iter 10989 Ep: 2.98 loss 1.935 score 0.591 lr 3.78215e-05 
09/13/2020 09:58:50 - INFO - volta.utils -   [GQA]: iter 11009 Ep: 2.99 loss 1.899 score 0.613 lr 3.78094e-05 
09/13/2020 10:01:51 - INFO - volta.utils -   [GQA]: iter 11029 Ep: 2.99 loss 1.889 score 0.609 lr 3.77974e-05 
09/13/2020 10:02:43 - INFO - volta.utils -   [GQA]: iter 11049 Ep: 3.00 loss 1.893 score 0.619 lr 3.77853e-05 
09/13/2020 10:02:45 - INFO - __main__ -   ** ** * Saving fine - tuned model ** ** * 

Epoch:  15%|█▌        | 3/20 [17:05:43<97:16:07, 20598.10s/it] 09/13/2020 10:22:55 - INFO - volta.utils -   Eval task TASK15 on iteration 11053 
09/13/2020 10:22:55 - INFO - volta.utils -   Validation [GQA]: loss 1.956 score 59.854 
09/13/2020 10:23:07 - INFO - volta.utils -   [GQA]: iter 11073 Ep: 3.01 loss 1.802 score 0.636 lr 3.7772e-05 
09/13/2020 10:24:12 - INFO - volta.utils -   [GQA]: iter 11093 Ep: 3.01 loss 1.845 score 0.636 lr 3.77588e-05 
09/13/2020 10:25:10 - INFO - volta.utils -   [GQA]: iter 11113 Ep: 3.02 loss 1.820 score 0.635 lr 3.77467e-05 
09/13/2020 10:26:31 - INFO - volta.utils -   [GQA]: iter 11133 Ep: 3.02 loss 1.799 score 0.640 lr 3.77346e-05 
09/13/2020 10:28:03 - INFO - volta.utils -   [GQA]: iter 11153 Ep: 3.03 loss 1.807 score 0.637 lr 3.77226e-05 
09/13/2020 10:29:33 - INFO - volta.utils -   [GQA]: iter 11173 Ep: 3.03 loss 1.756 score 0.653 lr 3.77105e-05 
09/13/2020 10:31:16 - INFO - volta.utils -   [GQA]: iter 11193 Ep: 3.04 loss 1.791 score 0.638 lr 3.76985e-05 
09/13/2020 10:32:17 - INFO - volta.utils -   [GQA]: iter 11213 Ep: 3.04 loss 1.827 score 0.629 lr 3.76864e-05 
09/13/2020 10:34:28 - INFO - volta.utils -   [GQA]: iter 11233 Ep: 3.05 loss 1.753 score 0.642 lr 3.76743e-05 
09/13/2020 10:37:01 - INFO - volta.utils -   [GQA]: iter 11253 Ep: 3.05 loss 1.757 score 0.643 lr 3.76623e-05 
09/13/2020 10:38:34 - INFO - volta.utils -   [GQA]: iter 11273 Ep: 3.06 loss 1.808 score 0.630 lr 3.76502e-05 
09/13/2020 10:40:14 - INFO - volta.utils -   [GQA]: iter 11293 Ep: 3.07 loss 1.774 score 0.642 lr 3.76381e-05 
09/13/2020 10:42:27 - INFO - volta.utils -   [GQA]: iter 11313 Ep: 3.07 loss 1.765 score 0.637 lr 3.76261e-05 
09/13/2020 10:44:43 - INFO - volta.utils -   [GQA]: iter 11333 Ep: 3.08 loss 1.829 score 0.632 lr 3.7614e-05 
09/13/2020 10:46:25 - INFO - volta.utils -   [GQA]: iter 11353 Ep: 3.08 loss 1.816 score 0.637 lr 3.76019e-05 
09/13/2020 10:47:55 - INFO - volta.utils -   [GQA]: iter 11373 Ep: 3.09 loss 1.780 score 0.631 lr 3.75899e-05 
09/13/2020 10:49:36 - INFO - volta.utils -   [GQA]: iter 11393 Ep: 3.09 loss 1.777 score 0.643 lr 3.75778e-05 
09/13/2020 10:52:36 - INFO - volta.utils -   [GQA]: iter 11413 Ep: 3.10 loss 1.791 score 0.637 lr 3.75657e-05 
09/13/2020 10:54:07 - INFO - volta.utils -   [GQA]: iter 11433 Ep: 3.10 loss 1.714 score 0.645 lr 3.75537e-05 
09/13/2020 10:55:37 - INFO - volta.utils -   [GQA]: iter 11453 Ep: 3.11 loss 1.752 score 0.646 lr 3.75416e-05 
09/13/2020 10:57:32 - INFO - volta.utils -   [GQA]: iter 11473 Ep: 3.11 loss 1.780 score 0.638 lr 3.75296e-05 
09/13/2020 10:59:58 - INFO - volta.utils -   [GQA]: iter 11493 Ep: 3.12 loss 1.733 score 0.637 lr 3.75175e-05 
09/13/2020 11:01:43 - INFO - volta.utils -   [GQA]: iter 11513 Ep: 3.13 loss 1.752 score 0.643 lr 3.75054e-05 
09/13/2020 11:03:15 - INFO - volta.utils -   [GQA]: iter 11533 Ep: 3.13 loss 1.777 score 0.637 lr 3.74934e-05 
09/13/2020 11:05:00 - INFO - volta.utils -   [GQA]: iter 11553 Ep: 3.14 loss 1.727 score 0.650 lr 3.74813e-05 
09/13/2020 11:08:00 - INFO - volta.utils -   [GQA]: iter 11573 Ep: 3.14 loss 1.798 score 0.639 lr 3.74692e-05 
09/13/2020 11:09:32 - INFO - volta.utils -   [GQA]: iter 11593 Ep: 3.15 loss 1.741 score 0.647 lr 3.74572e-05 
09/13/2020 11:10:49 - INFO - volta.utils -   [GQA]: iter 11613 Ep: 3.15 loss 1.782 score 0.638 lr 3.74451e-05 
09/13/2020 11:11:56 - INFO - volta.utils -   [GQA]: iter 11633 Ep: 3.16 loss 1.718 score 0.653 lr 3.7433e-05 
09/13/2020 11:14:56 - INFO - volta.utils -   [GQA]: iter 11653 Ep: 3.16 loss 1.773 score 0.639 lr 3.7421e-05 
09/13/2020 11:16:40 - INFO - volta.utils -   [GQA]: iter 11673 Ep: 3.17 loss 1.788 score 0.634 lr 3.74089e-05 
09/13/2020 11:18:39 - INFO - volta.utils -   [GQA]: iter 11693 Ep: 3.17 loss 1.767 score 0.635 lr 3.73969e-05 
09/13/2020 11:20:11 - INFO - volta.utils -   [GQA]: iter 11713 Ep: 3.18 loss 1.727 score 0.650 lr 3.73848e-05 
09/13/2020 11:22:27 - INFO - volta.utils -   [GQA]: iter 11733 Ep: 3.18 loss 1.787 score 0.637 lr 3.73727e-05 
09/13/2020 11:24:27 - INFO - volta.utils -   [GQA]: iter 11753 Ep: 3.19 loss 1.758 score 0.639 lr 3.73607e-05 
09/13/2020 11:25:26 - INFO - volta.utils -   [GQA]: iter 11773 Ep: 3.20 loss 1.765 score 0.639 lr 3.73486e-05 
09/13/2020 11:27:16 - INFO - volta.utils -   [GQA]: iter 11793 Ep: 3.20 loss 1.747 score 0.645 lr 3.73365e-05 
09/13/2020 11:28:16 - INFO - volta.utils -   [GQA]: iter 11813 Ep: 3.21 loss 1.748 score 0.650 lr 3.73245e-05 
09/13/2020 11:30:49 - INFO - volta.utils -   [GQA]: iter 11833 Ep: 3.21 loss 1.782 score 0.641 lr 3.73124e-05 
09/13/2020 11:32:12 - INFO - volta.utils -   [GQA]: iter 11853 Ep: 3.22 loss 1.729 score 0.644 lr 3.73003e-05 
09/13/2020 11:33:46 - INFO - volta.utils -   [GQA]: iter 11873 Ep: 3.22 loss 1.755 score 0.642 lr 3.72883e-05 
09/13/2020 11:35:12 - INFO - volta.utils -   [GQA]: iter 11893 Ep: 3.23 loss 1.768 score 0.634 lr 3.72762e-05 
09/13/2020 11:37:08 - INFO - volta.utils -   [GQA]: iter 11913 Ep: 3.23 loss 1.733 score 0.641 lr 3.72641e-05 
09/13/2020 11:38:18 - INFO - volta.utils -   [GQA]: iter 11933 Ep: 3.24 loss 1.742 score 0.644 lr 3.72521e-05 
09/13/2020 11:39:46 - INFO - volta.utils -   [GQA]: iter 11953 Ep: 3.24 loss 1.776 score 0.647 lr 3.724e-05 
09/13/2020 11:41:39 - INFO - volta.utils -   [GQA]: iter 11973 Ep: 3.25 loss 1.718 score 0.649 lr 3.7228e-05 
09/13/2020 11:43:36 - INFO - volta.utils -   [GQA]: iter 11993 Ep: 3.26 loss 1.741 score 0.653 lr 3.72159e-05 
09/13/2020 11:45:07 - INFO - volta.utils -   [GQA]: iter 12013 Ep: 3.26 loss 1.739 score 0.651 lr 3.72038e-05 
09/13/2020 11:46:29 - INFO - volta.utils -   [GQA]: iter 12033 Ep: 3.27 loss 1.756 score 0.641 lr 3.71918e-05 
09/13/2020 11:48:44 - INFO - volta.utils -   [GQA]: iter 12053 Ep: 3.27 loss 1.716 score 0.655 lr 3.71797e-05 
09/13/2020 11:50:43 - INFO - volta.utils -   [GQA]: iter 12073 Ep: 3.28 loss 1.679 score 0.655 lr 3.71676e-05 
09/13/2020 11:52:35 - INFO - volta.utils -   [GQA]: iter 12093 Ep: 3.28 loss 1.702 score 0.666 lr 3.71556e-05 
09/13/2020 11:54:15 - INFO - volta.utils -   [GQA]: iter 12113 Ep: 3.29 loss 1.737 score 0.647 lr 3.71435e-05 
09/13/2020 11:56:52 - INFO - volta.utils -   [GQA]: iter 12133 Ep: 3.29 loss 1.747 score 0.639 lr 3.71314e-05 
09/13/2020 11:58:49 - INFO - volta.utils -   [GQA]: iter 12153 Ep: 3.30 loss 1.701 score 0.655 lr 3.71194e-05 
09/13/2020 11:59:47 - INFO - volta.utils -   [GQA]: iter 12173 Ep: 3.30 loss 1.738 score 0.653 lr 3.71073e-05 
09/13/2020 12:01:46 - INFO - volta.utils -   [GQA]: iter 12193 Ep: 3.31 loss 1.736 score 0.654 lr 3.70952e-05 
09/13/2020 12:03:46 - INFO - volta.utils -   [GQA]: iter 12213 Ep: 3.32 loss 1.717 score 0.649 lr 3.70832e-05 
09/13/2020 12:05:54 - INFO - volta.utils -   [GQA]: iter 12233 Ep: 3.32 loss 1.696 score 0.657 lr 3.70711e-05 
09/13/2020 12:07:59 - INFO - volta.utils -   [GQA]: iter 12253 Ep: 3.33 loss 1.688 score 0.652 lr 3.70591e-05 
09/13/2020 12:09:58 - INFO - volta.utils -   [GQA]: iter 12273 Ep: 3.33 loss 1.732 score 0.647 lr 3.7047e-05 
09/13/2020 12:11:38 - INFO - volta.utils -   [GQA]: iter 12293 Ep: 3.34 loss 1.726 score 0.636 lr 3.70349e-05 
09/13/2020 12:13:09 - INFO - volta.utils -   [GQA]: iter 12313 Ep: 3.34 loss 1.776 score 0.637 lr 3.70229e-05 
09/13/2020 12:14:44 - INFO - volta.utils -   [GQA]: iter 12333 Ep: 3.35 loss 1.728 score 0.654 lr 3.70108e-05 
09/13/2020 12:17:13 - INFO - volta.utils -   [GQA]: iter 12353 Ep: 3.35 loss 1.743 score 0.642 lr 3.69987e-05 
09/13/2020 12:18:54 - INFO - volta.utils -   [GQA]: iter 12373 Ep: 3.36 loss 1.747 score 0.646 lr 3.69867e-05 
09/13/2020 12:20:37 - INFO - volta.utils -   [GQA]: iter 12393 Ep: 3.36 loss 1.690 score 0.658 lr 3.69746e-05 
09/13/2020 12:22:22 - INFO - volta.utils -   [GQA]: iter 12413 Ep: 3.37 loss 1.729 score 0.646 lr 3.69625e-05 
09/13/2020 12:24:18 - INFO - volta.utils -   [GQA]: iter 12433 Ep: 3.37 loss 1.691 score 0.652 lr 3.69505e-05 
09/13/2020 12:25:29 - INFO - volta.utils -   [GQA]: iter 12453 Ep: 3.38 loss 1.645 score 0.665 lr 3.69384e-05 
09/13/2020 12:26:54 - INFO - volta.utils -   [GQA]: iter 12473 Ep: 3.39 loss 1.714 score 0.648 lr 3.69263e-05 
09/13/2020 12:29:03 - INFO - volta.utils -   [GQA]: iter 12493 Ep: 3.39 loss 1.708 score 0.644 lr 3.69143e-05 
09/13/2020 12:31:47 - INFO - volta.utils -   [GQA]: iter 12513 Ep: 3.40 loss 1.692 score 0.654 lr 3.69022e-05 
09/13/2020 12:33:25 - INFO - volta.utils -   [GQA]: iter 12533 Ep: 3.40 loss 1.730 score 0.655 lr 3.68902e-05 
09/13/2020 12:34:48 - INFO - volta.utils -   [GQA]: iter 12553 Ep: 3.41 loss 1.680 score 0.662 lr 3.68781e-05 
09/13/2020 12:36:31 - INFO - volta.utils -   [GQA]: iter 12573 Ep: 3.41 loss 1.700 score 0.656 lr 3.6866e-05 
09/13/2020 12:38:22 - INFO - volta.utils -   [GQA]: iter 12593 Ep: 3.42 loss 1.707 score 0.654 lr 3.6854e-05 
09/13/2020 12:39:50 - INFO - volta.utils -   [GQA]: iter 12613 Ep: 3.42 loss 1.707 score 0.649 lr 3.68419e-05 
09/13/2020 12:41:24 - INFO - volta.utils -   [GQA]: iter 12633 Ep: 3.43 loss 1.709 score 0.657 lr 3.68298e-05 
09/13/2020 12:43:15 - INFO - volta.utils -   [GQA]: iter 12653 Ep: 3.43 loss 1.736 score 0.644 lr 3.68178e-05 
09/13/2020 12:44:57 - INFO - volta.utils -   [GQA]: iter 12673 Ep: 3.44 loss 1.713 score 0.660 lr 3.68057e-05 
09/13/2020 12:46:30 - INFO - volta.utils -   [GQA]: iter 12693 Ep: 3.45 loss 1.701 score 0.646 lr 3.67936e-05 
09/13/2020 12:48:10 - INFO - volta.utils -   [GQA]: iter 12713 Ep: 3.45 loss 1.729 score 0.648 lr 3.67816e-05 
09/13/2020 12:50:20 - INFO - volta.utils -   [GQA]: iter 12733 Ep: 3.46 loss 1.695 score 0.653 lr 3.67695e-05 
09/13/2020 12:51:40 - INFO - volta.utils -   [GQA]: iter 12753 Ep: 3.46 loss 1.714 score 0.650 lr 3.67574e-05 
09/13/2020 12:53:13 - INFO - volta.utils -   [GQA]: iter 12773 Ep: 3.47 loss 1.722 score 0.644 lr 3.67454e-05 
09/13/2020 12:54:53 - INFO - volta.utils -   [GQA]: iter 12793 Ep: 3.47 loss 1.686 score 0.654 lr 3.67333e-05 
09/13/2020 12:56:36 - INFO - volta.utils -   [GQA]: iter 12813 Ep: 3.48 loss 1.673 score 0.657 lr 3.67213e-05 
09/13/2020 12:58:13 - INFO - volta.utils -   [GQA]: iter 12833 Ep: 3.48 loss 1.768 score 0.644 lr 3.67092e-05 
09/13/2020 12:59:03 - INFO - volta.utils -   [GQA]: iter 12853 Ep: 3.49 loss 1.706 score 0.654 lr 3.66971e-05 
09/13/2020 13:01:13 - INFO - volta.utils -   [GQA]: iter 12873 Ep: 3.49 loss 1.696 score 0.665 lr 3.66851e-05 
09/13/2020 13:02:50 - INFO - volta.utils -   [GQA]: iter 12893 Ep: 3.50 loss 1.736 score 0.648 lr 3.6673e-05 
09/13/2020 13:04:19 - INFO - volta.utils -   [GQA]: iter 12913 Ep: 3.51 loss 1.685 score 0.661 lr 3.66609e-05 
09/13/2020 13:05:38 - INFO - volta.utils -   [GQA]: iter 12933 Ep: 3.51 loss 1.679 score 0.651 lr 3.66489e-05 
09/13/2020 13:07:42 - INFO - volta.utils -   [GQA]: iter 12953 Ep: 3.52 loss 1.646 score 0.669 lr 3.66368e-05 
09/13/2020 13:10:17 - INFO - volta.utils -   [GQA]: iter 12973 Ep: 3.52 loss 1.648 score 0.662 lr 3.66247e-05 
09/13/2020 13:11:27 - INFO - volta.utils -   [GQA]: iter 12993 Ep: 3.53 loss 1.662 score 0.665 lr 3.66127e-05 
09/13/2020 13:13:17 - INFO - volta.utils -   [GQA]: iter 13013 Ep: 3.53 loss 1.658 score 0.662 lr 3.66006e-05 
09/13/2020 13:14:58 - INFO - volta.utils -   [GQA]: iter 13033 Ep: 3.54 loss 1.695 score 0.650 lr 3.65886e-05 
09/13/2020 13:18:02 - INFO - volta.utils -   [GQA]: iter 13053 Ep: 3.54 loss 1.727 score 0.651 lr 3.65765e-05 
09/13/2020 13:19:29 - INFO - volta.utils -   [GQA]: iter 13073 Ep: 3.55 loss 1.689 score 0.660 lr 3.65644e-05 
09/13/2020 13:21:17 - INFO - volta.utils -   [GQA]: iter 13093 Ep: 3.55 loss 1.651 score 0.673 lr 3.65524e-05 
09/13/2020 13:23:11 - INFO - volta.utils -   [GQA]: iter 13113 Ep: 3.56 loss 1.662 score 0.667 lr 3.65403e-05 
09/13/2020 13:24:44 - INFO - volta.utils -   [GQA]: iter 13133 Ep: 3.56 loss 1.665 score 0.656 lr 3.65282e-05 
09/13/2020 13:26:41 - INFO - volta.utils -   [GQA]: iter 13153 Ep: 3.57 loss 1.666 score 0.667 lr 3.65162e-05 
09/13/2020 13:29:13 - INFO - volta.utils -   [GQA]: iter 13173 Ep: 3.58 loss 1.682 score 0.661 lr 3.65041e-05 
09/13/2020 13:30:37 - INFO - volta.utils -   [GQA]: iter 13193 Ep: 3.58 loss 1.643 score 0.665 lr 3.6492e-05 
09/13/2020 13:31:42 - INFO - volta.utils -   [GQA]: iter 13213 Ep: 3.59 loss 1.653 score 0.663 lr 3.648e-05 
09/13/2020 13:33:26 - INFO - volta.utils -   [GQA]: iter 13233 Ep: 3.59 loss 1.614 score 0.675 lr 3.64679e-05 
09/13/2020 13:36:14 - INFO - volta.utils -   [GQA]: iter 13253 Ep: 3.60 loss 1.659 score 0.663 lr 3.64558e-05 
09/13/2020 13:37:43 - INFO - volta.utils -   [GQA]: iter 13273 Ep: 3.60 loss 1.685 score 0.662 lr 3.64438e-05 
09/13/2020 13:39:42 - INFO - volta.utils -   [GQA]: iter 13293 Ep: 3.61 loss 1.683 score 0.652 lr 3.64317e-05 
09/13/2020 13:42:21 - INFO - volta.utils -   [GQA]: iter 13313 Ep: 3.61 loss 1.667 score 0.653 lr 3.64197e-05 
09/13/2020 13:43:57 - INFO - volta.utils -   [GQA]: iter 13333 Ep: 3.62 loss 1.667 score 0.654 lr 3.64076e-05 
09/13/2020 13:45:14 - INFO - volta.utils -   [GQA]: iter 13353 Ep: 3.62 loss 1.665 score 0.664 lr 3.63955e-05 
09/13/2020 13:46:45 - INFO - volta.utils -   [GQA]: iter 13373 Ep: 3.63 loss 1.606 score 0.667 lr 3.63835e-05 
09/13/2020 13:49:37 - INFO - volta.utils -   [GQA]: iter 13393 Ep: 3.64 loss 1.666 score 0.655 lr 3.63714e-05 
09/13/2020 13:50:51 - INFO - volta.utils -   [GQA]: iter 13413 Ep: 3.64 loss 1.594 score 0.668 lr 3.63593e-05 
09/13/2020 13:51:48 - INFO - volta.utils -   [GQA]: iter 13433 Ep: 3.65 loss 1.643 score 0.670 lr 3.63473e-05 
09/13/2020 13:53:21 - INFO - volta.utils -   [GQA]: iter 13453 Ep: 3.65 loss 1.647 score 0.661 lr 3.63352e-05 
09/13/2020 13:55:03 - INFO - volta.utils -   [GQA]: iter 13473 Ep: 3.66 loss 1.682 score 0.659 lr 3.63231e-05 
09/13/2020 13:56:17 - INFO - volta.utils -   [GQA]: iter 13493 Ep: 3.66 loss 1.692 score 0.658 lr 3.63111e-05 
09/13/2020 13:57:57 - INFO - volta.utils -   [GQA]: iter 13513 Ep: 3.67 loss 1.651 score 0.662 lr 3.6299e-05 
09/13/2020 13:59:58 - INFO - volta.utils -   [GQA]: iter 13533 Ep: 3.67 loss 1.649 score 0.663 lr 3.62869e-05 
09/13/2020 14:01:40 - INFO - volta.utils -   [GQA]: iter 13553 Ep: 3.68 loss 1.613 score 0.672 lr 3.62749e-05 
09/13/2020 14:02:57 - INFO - volta.utils -   [GQA]: iter 13573 Ep: 3.68 loss 1.622 score 0.669 lr 3.62628e-05 
09/13/2020 14:04:03 - INFO - volta.utils -   [GQA]: iter 13593 Ep: 3.69 loss 1.651 score 0.669 lr 3.62508e-05 
09/13/2020 14:06:45 - INFO - volta.utils -   [GQA]: iter 13613 Ep: 3.70 loss 1.637 score 0.668 lr 3.62387e-05 
09/13/2020 14:08:33 - INFO - volta.utils -   [GQA]: iter 13633 Ep: 3.70 loss 1.670 score 0.656 lr 3.62266e-05 
09/13/2020 14:10:03 - INFO - volta.utils -   [GQA]: iter 13653 Ep: 3.71 loss 1.679 score 0.655 lr 3.62146e-05 
09/13/2020 14:11:29 - INFO - volta.utils -   [GQA]: iter 13673 Ep: 3.71 loss 1.641 score 0.671 lr 3.62025e-05 
09/13/2020 14:13:36 - INFO - volta.utils -   [GQA]: iter 13693 Ep: 3.72 loss 1.646 score 0.666 lr 3.61904e-05 
09/13/2020 14:15:22 - INFO - volta.utils -   [GQA]: iter 13713 Ep: 3.72 loss 1.615 score 0.664 lr 3.61784e-05 
09/13/2020 14:16:47 - INFO - volta.utils -   [GQA]: iter 13733 Ep: 3.73 loss 1.605 score 0.675 lr 3.61663e-05 
09/13/2020 14:18:31 - INFO - volta.utils -   [GQA]: iter 13753 Ep: 3.73 loss 1.624 score 0.669 lr 3.61542e-05 
09/13/2020 14:21:09 - INFO - volta.utils -   [GQA]: iter 13773 Ep: 3.74 loss 1.597 score 0.676 lr 3.61422e-05 
09/13/2020 14:23:00 - INFO - volta.utils -   [GQA]: iter 13793 Ep: 3.74 loss 1.672 score 0.656 lr 3.61301e-05 
09/13/2020 14:24:05 - INFO - volta.utils -   [GQA]: iter 13813 Ep: 3.75 loss 1.686 score 0.653 lr 3.6118e-05 
09/13/2020 14:25:35 - INFO - volta.utils -   [GQA]: iter 13833 Ep: 3.75 loss 1.611 score 0.667 lr 3.6106e-05 
09/13/2020 14:28:02 - INFO - volta.utils -   [GQA]: iter 13853 Ep: 3.76 loss 1.678 score 0.664 lr 3.60939e-05 
09/13/2020 14:29:33 - INFO - volta.utils -   [GQA]: iter 13873 Ep: 3.77 loss 1.692 score 0.655 lr 3.60819e-05 
09/13/2020 14:31:28 - INFO - volta.utils -   [GQA]: iter 13893 Ep: 3.77 loss 1.629 score 0.665 lr 3.60698e-05 
09/13/2020 14:32:47 - INFO - volta.utils -   [GQA]: iter 13913 Ep: 3.78 loss 1.599 score 0.671 lr 3.60577e-05 
09/13/2020 14:35:54 - INFO - volta.utils -   [GQA]: iter 13933 Ep: 3.78 loss 1.608 score 0.675 lr 3.60457e-05 
09/13/2020 14:37:44 - INFO - volta.utils -   [GQA]: iter 13953 Ep: 3.79 loss 1.633 score 0.666 lr 3.60336e-05 
09/13/2020 14:39:10 - INFO - volta.utils -   [GQA]: iter 13973 Ep: 3.79 loss 1.647 score 0.668 lr 3.60215e-05 
09/13/2020 14:40:43 - INFO - volta.utils -   [GQA]: iter 13993 Ep: 3.80 loss 1.605 score 0.673 lr 3.60095e-05 
09/13/2020 14:43:01 - INFO - volta.utils -   [GQA]: iter 14013 Ep: 3.80 loss 1.615 score 0.668 lr 3.59974e-05 
09/13/2020 14:44:41 - INFO - volta.utils -   [GQA]: iter 14033 Ep: 3.81 loss 1.636 score 0.668 lr 3.59853e-05 
09/13/2020 14:46:22 - INFO - volta.utils -   [GQA]: iter 14053 Ep: 3.81 loss 1.653 score 0.659 lr 3.59733e-05 
09/13/2020 14:47:48 - INFO - volta.utils -   [GQA]: iter 14073 Ep: 3.82 loss 1.579 score 0.681 lr 3.59612e-05 
09/13/2020 14:50:32 - INFO - volta.utils -   [GQA]: iter 14093 Ep: 3.83 loss 1.613 score 0.670 lr 3.59491e-05 
09/13/2020 14:51:47 - INFO - volta.utils -   [GQA]: iter 14113 Ep: 3.83 loss 1.660 score 0.669 lr 3.59371e-05 
09/13/2020 14:53:50 - INFO - volta.utils -   [GQA]: iter 14133 Ep: 3.84 loss 1.665 score 0.661 lr 3.5925e-05 
09/13/2020 14:54:35 - INFO - volta.utils -   [GQA]: iter 14153 Ep: 3.84 loss 1.604 score 0.673 lr 3.5913e-05 
09/13/2020 14:56:56 - INFO - volta.utils -   [GQA]: iter 14173 Ep: 3.85 loss 1.629 score 0.666 lr 3.59009e-05 
09/13/2020 14:58:23 - INFO - volta.utils -   [GQA]: iter 14193 Ep: 3.85 loss 1.624 score 0.673 lr 3.58888e-05 
09/13/2020 15:00:29 - INFO - volta.utils -   [GQA]: iter 14213 Ep: 3.86 loss 1.563 score 0.679 lr 3.58768e-05 
09/13/2020 15:01:21 - INFO - volta.utils -   [GQA]: iter 14233 Ep: 3.86 loss 1.588 score 0.675 lr 3.58647e-05 
09/13/2020 15:03:23 - INFO - volta.utils -   [GQA]: iter 14253 Ep: 3.87 loss 1.590 score 0.683 lr 3.58526e-05 
09/13/2020 15:04:53 - INFO - volta.utils -   [GQA]: iter 14273 Ep: 3.87 loss 1.610 score 0.670 lr 3.58406e-05 
09/13/2020 15:06:27 - INFO - volta.utils -   [GQA]: iter 14293 Ep: 3.88 loss 1.583 score 0.683 lr 3.58285e-05 
09/13/2020 15:07:40 - INFO - volta.utils -   [GQA]: iter 14313 Ep: 3.89 loss 1.609 score 0.679 lr 3.58164e-05 
09/13/2020 15:09:53 - INFO - volta.utils -   [GQA]: iter 14333 Ep: 3.89 loss 1.591 score 0.671 lr 3.58044e-05 
09/13/2020 15:11:38 - INFO - volta.utils -   [GQA]: iter 14353 Ep: 3.90 loss 1.601 score 0.674 lr 3.57923e-05 
09/13/2020 15:13:13 - INFO - volta.utils -   [GQA]: iter 14373 Ep: 3.90 loss 1.612 score 0.665 lr 3.57803e-05 
09/13/2020 15:14:45 - INFO - volta.utils -   [GQA]: iter 14393 Ep: 3.91 loss 1.620 score 0.664 lr 3.57682e-05 
09/13/2020 15:15:36 - INFO - volta.utils -   [GQA]: iter 14413 Ep: 3.91 loss 1.574 score 0.672 lr 3.57561e-05 
09/13/2020 15:18:03 - INFO - volta.utils -   [GQA]: iter 14433 Ep: 3.92 loss 1.637 score 0.655 lr 3.57441e-05 
09/13/2020 15:19:08 - INFO - volta.utils -   [GQA]: iter 14453 Ep: 3.92 loss 1.580 score 0.676 lr 3.5732e-05 
09/13/2020 15:20:51 - INFO - volta.utils -   [GQA]: iter 14473 Ep: 3.93 loss 1.623 score 0.658 lr 3.57199e-05 
09/13/2020 15:22:25 - INFO - volta.utils -   [GQA]: iter 14493 Ep: 3.93 loss 1.633 score 0.672 lr 3.57079e-05 
09/13/2020 15:24:26 - INFO - volta.utils -   [GQA]: iter 14513 Ep: 3.94 loss 1.645 score 0.670 lr 3.56958e-05 
09/13/2020 15:25:47 - INFO - volta.utils -   [GQA]: iter 14533 Ep: 3.94 loss 1.600 score 0.678 lr 3.56837e-05 
09/13/2020 15:27:35 - INFO - volta.utils -   [GQA]: iter 14553 Ep: 3.95 loss 1.560 score 0.683 lr 3.56717e-05 
09/13/2020 15:28:54 - INFO - volta.utils -   [GQA]: iter 14573 Ep: 3.96 loss 1.627 score 0.667 lr 3.56596e-05 
09/13/2020 15:31:41 - INFO - volta.utils -   [GQA]: iter 14593 Ep: 3.96 loss 1.636 score 0.665 lr 3.56475e-05 
09/13/2020 15:33:04 - INFO - volta.utils -   [GQA]: iter 14613 Ep: 3.97 loss 1.606 score 0.675 lr 3.56355e-05 
09/13/2020 15:34:45 - INFO - volta.utils -   [GQA]: iter 14633 Ep: 3.97 loss 1.595 score 0.666 lr 3.56234e-05 
09/13/2020 15:36:35 - INFO - volta.utils -   [GQA]: iter 14653 Ep: 3.98 loss 1.590 score 0.670 lr 3.56114e-05 
09/13/2020 15:39:30 - INFO - volta.utils -   [GQA]: iter 14673 Ep: 3.98 loss 1.557 score 0.676 lr 3.55993e-05 
09/13/2020 15:40:46 - INFO - volta.utils -   [GQA]: iter 14693 Ep: 3.99 loss 1.640 score 0.662 lr 3.55872e-05 
09/13/2020 15:41:50 - INFO - volta.utils -   [GQA]: iter 14713 Ep: 3.99 loss 1.629 score 0.666 lr 3.55752e-05 
09/13/2020 15:43:09 - INFO - volta.utils -   [GQA]: iter 14733 Ep: 4.00 loss 1.560 score 0.684 lr 3.55631e-05 
09/13/2020 15:43:25 - INFO - __main__ -   ** ** * Saving fine - tuned model ** ** * 

Epoch:  20%|██        | 4/20 [22:46:24<91:20:16, 20551.01s/it]09/13/2020 16:03:38 - INFO - volta.utils -   Eval task TASK15 on iteration 14737 
09/13/2020 16:03:38 - INFO - volta.utils -   Validation [GQA]: loss 1.820 score 63.535 
09/13/2020 16:03:51 - INFO - volta.utils -   [GQA]: iter 14757 Ep: 4.01 loss 1.463 score 0.711 lr 3.55498e-05 
09/13/2020 16:05:28 - INFO - volta.utils -   [GQA]: iter 14777 Ep: 4.01 loss 1.419 score 0.717 lr 3.55366e-05 
09/13/2020 16:06:37 - INFO - volta.utils -   [GQA]: iter 14797 Ep: 4.02 loss 1.423 score 0.713 lr 3.55245e-05 
09/13/2020 16:08:06 - INFO - volta.utils -   [GQA]: iter 14817 Ep: 4.02 loss 1.424 score 0.718 lr 3.55124e-05 
09/13/2020 16:10:05 - INFO - volta.utils -   [GQA]: iter 14837 Ep: 4.03 loss 1.446 score 0.709 lr 3.55004e-05 
09/13/2020 16:11:15 - INFO - volta.utils -   [GQA]: iter 14857 Ep: 4.03 loss 1.418 score 0.711 lr 3.54883e-05 
09/13/2020 16:12:44 - INFO - volta.utils -   [GQA]: iter 14877 Ep: 4.04 loss 1.385 score 0.726 lr 3.54762e-05 
09/13/2020 16:14:07 - INFO - volta.utils -   [GQA]: iter 14897 Ep: 4.04 loss 1.391 score 0.727 lr 3.54642e-05 
09/13/2020 16:17:00 - INFO - volta.utils -   [GQA]: iter 14917 Ep: 4.05 loss 1.421 score 0.714 lr 3.54521e-05 
09/13/2020 16:18:24 - INFO - volta.utils -   [GQA]: iter 14937 Ep: 4.05 loss 1.439 score 0.709 lr 3.544e-05 
09/13/2020 16:20:09 - INFO - volta.utils -   [GQA]: iter 14957 Ep: 4.06 loss 1.409 score 0.714 lr 3.5428e-05 
09/13/2020 16:22:17 - INFO - volta.utils -   [GQA]: iter 14977 Ep: 4.07 loss 1.455 score 0.707 lr 3.54159e-05 
09/13/2020 16:25:26 - INFO - volta.utils -   [GQA]: iter 14997 Ep: 4.07 loss 1.453 score 0.700 lr 3.54038e-05 
09/13/2020 16:26:42 - INFO - volta.utils -   [GQA]: iter 15017 Ep: 4.08 loss 1.448 score 0.707 lr 3.53918e-05 
09/13/2020 16:27:52 - INFO - volta.utils -   [GQA]: iter 15037 Ep: 4.08 loss 1.447 score 0.711 lr 3.53797e-05 
09/13/2020 16:29:26 - INFO - volta.utils -   [GQA]: iter 15057 Ep: 4.09 loss 1.410 score 0.709 lr 3.53677e-05 
09/13/2020 16:32:04 - INFO - volta.utils -   [GQA]: iter 15077 Ep: 4.09 loss 1.460 score 0.704 lr 3.53556e-05 
09/13/2020 16:32:55 - INFO - volta.utils -   [GQA]: iter 15097 Ep: 4.10 loss 1.445 score 0.700 lr 3.53435e-05 
09/13/2020 16:34:09 - INFO - volta.utils -   [GQA]: iter 15117 Ep: 4.10 loss 1.414 score 0.718 lr 3.53315e-05 
09/13/2020 16:35:38 - INFO - volta.utils -   [GQA]: iter 15137 Ep: 4.11 loss 1.409 score 0.713 lr 3.53194e-05 
09/13/2020 16:38:54 - INFO - volta.utils -   [GQA]: iter 15157 Ep: 4.11 loss 1.441 score 0.704 lr 3.53073e-05 
09/13/2020 16:40:41 - INFO - volta.utils -   [GQA]: iter 15177 Ep: 4.12 loss 1.413 score 0.716 lr 3.52953e-05 
09/13/2020 16:42:12 - INFO - volta.utils -   [GQA]: iter 15197 Ep: 4.13 loss 1.442 score 0.717 lr 3.52832e-05 
09/13/2020 16:43:42 - INFO - volta.utils -   [GQA]: iter 15217 Ep: 4.13 loss 1.431 score 0.709 lr 3.52711e-05 
09/13/2020 16:46:08 - INFO - volta.utils -   [GQA]: iter 15237 Ep: 4.14 loss 1.394 score 0.723 lr 3.52591e-05 
09/13/2020 16:47:33 - INFO - volta.utils -   [GQA]: iter 15257 Ep: 4.14 loss 1.429 score 0.712 lr 3.5247e-05 
